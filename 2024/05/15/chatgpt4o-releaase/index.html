<!doctype html><html lang=zh><head><meta name=viewport content="width=device-width,initial-scale=1"><title>重磅！ChatGPT团队官宣船新版本的多模态大模型GPT-4o，完全免费！</title>
<meta charset=utf-8><meta name=google-adsense-account content="ca-pub-2871082647721658"><meta content="Web开发 ,Java ,Go ,Node.js ,PHP ,Koa ,MySQL ,Redis ,前端 ,后端 ,数据库" name=keywords><meta name=description content="上周，有传言表示OpenAI即将发布AI加持的搜索引擎来增强聊天机器人的功能并开拓新市场，不过Altman否认了该传言，并提到“不是GPT-5，也不是搜索引擎，但我们一直在努力开发我们认为人们会喜欢的东西！”
5月13日，ChatGPT团队官宣了最新旗舰大模型GPT-4o，可以实时对音频、视频和文本进行处理，完全免费，果然是人们会喜欢的东西！"><meta name=author content="Lei Xia"><link rel=canonical href=https://www.ddhigh.com/2024/05/15/chatgpt4o-releaase/><link rel=alternate type=application/rss+xml href=https://www.ddhigh.com//index.xml title=每天进步一点点><script async defer data-website-id=52f8f0f9-d93d-466b-8ef5-508aae8c4ed4 src=https://analysis.ddhigh.com/script.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-EC3XLVSGKV"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-EC3XLVSGKV")</script><meta property="og:url" content="https://www.ddhigh.com/2024/05/15/chatgpt4o-releaase/"><meta property="og:site_name" content="每天进步一点点"><meta property="og:title" content="重磅！ChatGPT团队官宣船新版本的多模态大模型GPT-4o，完全免费！"><meta property="og:description" content="上周，有传言表示OpenAI即将发布AI加持的搜索引擎来增强聊天机器人的功能并开拓新市场，不过Altman否认了该传言，并提到“不是GPT-5，也不是搜索引擎，但我们一直在努力开发我们认为人们会喜欢的东西！”
5月13日，ChatGPT团队官宣了最新旗舰大模型GPT-4o，可以实时对音频、视频和文本进行处理，完全免费，果然是人们会喜欢的东西！"><meta property="og:locale" content="zh"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-05-15T06:30:57+08:00"><meta property="article:modified_time" content="2024-05-15T06:30:57+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="重磅！ChatGPT团队官宣船新版本的多模态大模型GPT-4o，完全免费！"><meta name=twitter:description content="上周，有传言表示OpenAI即将发布AI加持的搜索引擎来增强聊天机器人的功能并开拓新市场，不过Altman否认了该传言，并提到“不是GPT-5，也不是搜索引擎，但我们一直在努力开发我们认为人们会喜欢的东西！”
5月13日，ChatGPT团队官宣了最新旗舰大模型GPT-4o，可以实时对音频、视频和文本进行处理，完全免费，果然是人们会喜欢的东西！"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://www.ddhigh.com/posts/"},{"@type":"ListItem","position":2,"name":"重磅！ChatGPT团队官宣船新版本的多模态大模型GPT-4o，完全免费！","item":"https://www.ddhigh.com/2024/05/15/chatgpt4o-releaase/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"重磅！ChatGPT团队官宣船新版本的多模态大模型GPT-4o，完全免费！","name":"重磅！ChatGPT团队官宣船新版本的多模态大模型GPT-4o，完全免费！","description":"上周，有传言表示OpenAI即将发布AI加持的搜索引擎来增强聊天机器人的功能并开拓新市场，不过Altman否认了该传言，并提到“不是GPT-5，也不是搜索引擎，但我们一直在努力开发我们认为人们会喜欢的东西！”\n5月13日，ChatGPT团队官宣了最新旗舰大模型GPT-4o，可以实时对音频、视频和文本进行处理，完全免费，果然是人们会喜欢的东西！\n","keywords":[],"articleBody":"上周，有传言表示OpenAI即将发布AI加持的搜索引擎来增强聊天机器人的功能并开拓新市场，不过Altman否认了该传言，并提到“不是GPT-5，也不是搜索引擎，但我们一直在努力开发我们认为人们会喜欢的东西！”\n5月13日，ChatGPT团队官宣了最新旗舰大模型GPT-4o，可以实时对音频、视频和文本进行处理，完全免费，果然是人们会喜欢的东西！\n完全免费的GPT-4o GPT-4o（“o”代表“omni”）是迈向更自然的人机交互的一步——它接受文本、音频和图像的任意组合作为输入，并生成文本、音频和图像的任意组合输出。 它可以在短至 232 毫秒的时间内响应音频输入，平均为 320 毫秒，这与人类在对话中的响应时间(opens in a new window)相似。 它在英语文本和代码上的性能与 GPT-4 Turbo 的性能相匹配，在非英语文本上的性能显着提高，同时 API 的速度也更快，成本降低了 50%。 与现有模型相比，GPT-4o 在视觉和音频理解方面尤其出色。\n在 GPT-4o 之前，可以使用语音模式与 ChatGPT 对话，平均延迟为 2.8 秒 (GPT-3.5) 和 5.4 秒 (GPT-4)。 为了实现这一目标，语音模式是由三个独立模型组成的管道：一个简单模型将音频转录为文本，GPT-3.5 或 GPT-4 接收文本并输出文本，第三个简单模型将该文本转换回音频。 这个过程意味着GPT-4丢失了大量信息——它无法直接观察音调、多个说话者或背景噪音，也无法输出笑声、歌唱或表达情感。\n借助 GPT-4o，OpenAI跨文本、视觉和音频端到端地训练了一个新模型，这意味着所有输入和输出都由同一神经网络处理。 由于 GPT-4o 是OpenAI第一个结合所有这些模式的模型，因此OpenAI仍然只是浅尝辄止地探索该模型的功能及其局限性。\n能力展示 OpenAI在官网展示了非常多的GPT-4o的Demo，可以自由地在音频、视频和文本间进行转换，而且信息不失真。\n我们一起来看一下文本生成图片的例子：\nA first person view of a robot typewriting the following journal entries:\nyo, so like, i can see now?? caught the sunrise and it was insane, colors everywhere. kinda makes you wonder, like, what even is reality? the text is large, legible and clear. the robot’s hands type on the typewriter.\n（翻译）\n机器人正在打字的第一人称视角如下日记条目：\n1.哟，这么喜欢，我现在可以看到了吗？ 赶上了日出，真是太疯狂了，到处都是色彩。 有点让你想知道，现实到底是什么？\n文字大、清晰易读。 机器人的手在打字机上打字。\nThe robot wrote the second entry. The page is now taller. The page has moved up. There are two entries on the sheet:\nyo, so like, i can see now?? caught the sunrise and it was insane, colors everywhere. kinda makes you wonder, like, what even is reality?\nsound update just dropped, and it’s wild. everything’s got a vibe now, every sound’s like a new secret. makes you think, what else am i missing?\n（翻译）\n机器人写下了第二个条目。 页面现在更高了。 页面已上移。 该表上有两个条目：\n哟，就像，我现在可以看到了？ 赶上了日出，真是太疯狂了，到处都是色彩。 有点让你想知道，现实到底是什么？\n声音更新刚刚下降，而且很疯狂。 现在一切都充满了活力，每一个声音都像是一个新的秘密。 让你思考，我还缺少什么？\nThe robot was unhappy with the writing so he is going to rip the sheet of paper. Here is his first person view as he rips it from top to bottom with his hands. The two halves are still legible and clear as he rips the sheet.\n（翻译）\n机器人对所写的内容不满意，所以他要撕掉那张纸。 这是他用手从上到下撕开它时的第一人称视角。 当他撕开纸张时，两半仍然清晰可见。\n可以看到GPT-4o在语义理解和绘图上确实进步一大截！下面我们来看看GPT-4o的性能。\n性能 改进推理 改进推理 - GPT-4o 在 0-shot COT MMLU（常识问题）上创下了 88.7% 的新高分。 所有这些评估都是通过新的简单评估（在新窗口中打开）库收集的。 此外，在传统的5-shot no-CoT MMLU上，GPT-4o创下了87.2%的新高分。 （注：Llama3 400b(opens in a new window)仍在训练中）\n语音识别 语音识别性能 - GPT-4o 比 Whisper-v3 显着提高了所有语言的语音识别性能，特别是对于资源匮乏的语言。\n语音翻译 语言翻译性能 - GPT-4o 在语音翻译方面树立了新的最先进水平，并且在 MLS 基准测试中优于 Whisper-v3。\nM3Exam(多语言+视觉) M3Exam - M3Exam 基准测试既是多语言评估也是视觉评估，由来自其他国家标准化测试的多项选择题组成，有时还包括图形和图表。 在所有语言的基准测试中，GPT-4o 都比 GPT-4 更强。 （省略了斯瓦希里语和爪哇语的视力结果，因为这些语言的视力问题只有 5 个或更少。）\n视觉理解 视觉理解评估 - GPT-4o 在视觉感知基准上实现了最先进的性能。 所有视觉评估都是 0-shot，其中 MMMU、MathVista 和 ChartQA 作为 0-shot CoT。\n模型安全和局限性 GPT-4o 通过过滤训练数据和通过训练后细化模型行为等技术，在跨模式设计中内置了安全性。 OpenAI还创建了新的安全系统，为语音输出提供防护。\nOpenAI根据准备框架并按照自愿承诺评估了 GPT-4o。 团队对网络安全、CBRN、说服力和模型自主性的评估表明，GPT-4o 在这些类别中的任何类别中的得分都不高于中等风险。 该评估涉及在整个模型训练过程中运行一套自动化和人工评估。 OpenAI使用自定义微调和提示测试了模型的安全缓解前和安全缓解后版本，以更好地激发模型功能。\nGPT-4o 还与社会心理学、偏见和公平以及错误信息等领域的 70 多名外部专家进行了广泛的外部红队合作，以识别新添加的模式引入或放大的风险。 并且利用这些经验来制定安全干预措施，以提高与 GPT-4o 交互的安全性。\nOpenAI已经认识到 GPT-4o 的音频模式带来了各种新的风险。 然而团队还是选择公开发布文本和图像输入以及文本输出。 在接下来的几周和几个月里，OpenAI表示将致力于技术基础设施、培训后的可用性以及发布其他模式所需的安全性。 例如，在发布时，音频输出将仅限于选择预设的声音，并将遵守现有的安全政策。\n使用模型 GPT-4o 是OpenAI突破深度学习界限的最新举措，这次是朝着实用性的方向发展。 在过去的两年里，他们花费了大量的精力来提高堆栈每一层的效率。 作为这项研究的第一个成果，该团队能够更广泛地提供 GPT-4 级别模型。 GPT-4o 的功能将迭代推出。\nGPT-4o 的文本和图像功能已经开始在 ChatGPT 中推出。 而且OpenAI表示正在免费套餐中提供 GPT-4o，并向 Plus 用户提供高达 5 倍的消息限制。 后续将在未来几周内在 ChatGPT Plus 中推出新版语音模式 GPT-4o 的 alpha 版。\n开发人员现在还可以在 API 中访问 GPT-4o 作为文本和视觉模型。 与 GPT-4 Turbo 相比，GPT-4o 速度提高 2 倍，价格降低一半，速率限制提高 5 倍。 不过OpenAI表示计划在未来几周内在 API 中向一小部分值得信赖的合作伙伴推出对 GPT-4o 新音频和视频功能的支持，而不是全员开放API权限。\n","wordCount":"372","inLanguage":"zh","datePublished":"2024-05-15T06:30:57+08:00","dateModified":"2024-05-15T06:30:57+08:00","author":{"@type":"Person","name":"Lei Xia"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.ddhigh.com/2024/05/15/chatgpt4o-releaase/"},"publisher":{"@type":"Organization","name":"每天进步一点点","logo":{"@type":"ImageObject","url":"https://www.ddhigh.com/favicon.ico"}}}</script><link rel=icon href=/img/favicon.ico sizes=16x16><link rel=apple-touch-icon href=/img/favicon.ico><link rel=manifest href=/img/favicon.ico><link rel=stylesheet href=https://cdn.staticfile.org/lxgw-wenkai-webfont/1.6.0/style.css><link rel=stylesheet href=/css/main.min.d0df733b91ef046970ecd63de7aa626886fd263fd3fb2b8ff28b560e505831f7.css integrity="sha256-0N9zO5HvBGlw7NY956piaIb9Jj/T+yuP8otWDlBYMfc=" crossorigin=anonymous media=screen><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/css/share.min.css integrity="sha512-ygEyjMC6rqnzJqWGjRTJUPYMEs9JUOm3i7OWUS9CgQ4XkBUvMsgCS1I8JqavidQ2ClHcREB7IbA2mN08+r9Elg==" crossorigin=anonymous referrerpolicy=no-referrer><link rel=stylesheet href=/scss/highlight/github-dark.min.min.66034289ee9a113219a2c4aae0a8bd2095ab255c832a42efcf5863f10814e7a1.css><script src=/js/highlight.min.min.c607d6febd16934a82eb61d3a896ed9d869f54373cc63ce95864ed5488fe3128.js></script><script>hljs.highlightAll()</script><script>$darkModeInit.Content|safeJS</script><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2871082647721658" crossorigin=anonymous></script></head><body><main class=wrapper><nav class=navigation><section class=container><a class=navigation-brand href=/>每天进步一点点
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><span></span><span></span><span></span></label><ul class=navigation-list id=navigation-list><li class="navigation-item navigation-menu"><a class=navigation-link href=/>首页</a></li><li class="navigation-item navigation-menu"><a class=navigation-link href=/archives>归档</a></li><li class="navigation-item navigation-menu"><a class=navigation-link href=/books>出版物</a></li><li class="navigation-item navigation-menu"><a class=navigation-link href=/guestbook>留言板</a></li><li class="navigation-item menu-separator"><span>|</span></li><li class="navigation-item navigation-social"><a class=navigation-link href=https://github.com/xialeistudio><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-github"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a></li><li class="navigation-item navigation-dark"><button id=mode type=button aria-label="toggle user light or dark theme">
<span class=toggle-dark><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-moon"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg></span>
<span class=toggle-light><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-sun"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></span></button></li><li class="navigation-item navigation-language"><a href=https://www.ddhigh.com/en/>EN</a></li></ul></section></nav><div id=content><article class=blog-single><header class=blog-title><h1>重磅！ChatGPT团队官宣船新版本的多模态大模型GPT-4o，完全免费！</h1></header><p><small>2024年5月15日&nbsp;· 372 字&nbsp;· 2 分钟</small><p><div class=blog-toc><nav id=TableOfContents><ul><li><a href=#完全免费的gpt-4o>完全免费的GPT-4o</a></li><li><a href=#能力展示>能力展示</a></li><li><a href=#性能>性能</a><ul><li><a href=#改进推理>改进推理</a></li><li><a href=#语音识别>语音识别</a></li><li><a href=#语音翻译>语音翻译</a></li><li><a href=#m3exam多语言视觉>M3Exam(多语言+视觉)</a></li><li><a href=#视觉理解>视觉理解</a></li></ul></li><li><a href=#模型安全和局限性>模型安全和局限性</a></li><li><a href=#使用模型>使用模型</a></li></ul></nav></div><section class=blog-content><p>上周，有传言表示OpenAI即将发布AI加持的搜索引擎来增强聊天机器人的功能并开拓新市场，不过Altman否认了该传言，并提到“不是GPT-5，也不是搜索引擎，但我们一直在努力开发我们认为人们会喜欢的东西！”</p><p>5月13日，ChatGPT团队官宣了最新旗舰大模型<strong>GPT-4o</strong>，可以实时对音频、视频和文本进行处理，完全免费，果然是人们会喜欢的东西！</p><h2 id=完全免费的gpt-4o>完全免费的GPT-4o</h2><p>GPT-4o（“o”代表“omni”）是迈向更自然的人机交互的一步——它接受文本、音频和图像的任意组合作为输入，并生成文本、音频和图像的任意组合输出。 它可以在<strong>短至 232 毫秒</strong>的时间内<strong>响应音频输入</strong>，平均为 320 毫秒，<strong>这与人类在对话中的响应时间(opens in a new window)相似</strong>。 它在英语文本和代码上的性能与 GPT-4 Turbo 的性能相匹配，在非英语文本上的性能显着提高，同时 API 的速度也更快，成本降低了 50%。 与现有模型相比，GPT-4o 在视觉和音频理解方面尤其出色。</p><p>在 GPT-4o 之前，可以使用语音模式与 ChatGPT 对话，<strong>平均延迟为 2.8 秒 (GPT-3.5) 和 5.4 秒 (GPT-4)</strong>。 为了实现这一目标，语音模式是由三个独立模型组成的管道：一个简单模型将音频转录为文本，GPT-3.5 或 GPT-4 接收文本并输出文本，第三个简单模型将该文本转换回音频。 这个过程意味着GPT-4丢失了大量信息——它无法直接观察音调、多个说话者或背景噪音，也无法输出笑声、歌唱或表达情感。</p><p>借助 GPT-4o，OpenAI跨文本、视觉和音频端到端地训练了一个新模型，这意味着所有输入和输出都由同一神经网络处理。 由于 GPT-4o 是OpenAI第一个结合所有这些模式的模型，因此OpenAI仍然只是浅尝辄止地探索该模型的功能及其局限性。</p><h2 id=能力展示>能力展示</h2><p>OpenAI在官网展示了非常多的GPT-4o的Demo，可以自由地在音频、视频和文本间进行转换，而且信息不失真。</p><p>我们一起来看一下文本生成图片的例子：</p><blockquote><p>A first person view of a robot typewriting the following journal entries:</p><ol><li>yo, so like, i can see now?? caught the sunrise and it was insane, colors everywhere. kinda makes you wonder, like, what even is reality?</li></ol><p>the text is large, legible and clear. the robot&rsquo;s hands type on the typewriter.</p><p>（翻译）</p><p>机器人正在打字的第一人称视角如下日记条目：</p><p>1.哟，这么喜欢，我现在可以看到了吗？ 赶上了日出，真是太疯狂了，到处都是色彩。 有点让你想知道，现实到底是什么？</p><p>文字大、清晰易读。 机器人的手在打字机上打字。</p></blockquote><p><img alt="Robot on typewriter" src=https://raw.githubusercontent.com/xialeistudio/picture-bucket/main/blog/robot-writers-block-01.jpg></p><blockquote><p>The robot wrote the second entry. The page is now taller. The page has moved up. There are two entries on the sheet:</p><p>yo, so like, i can see now?? caught the sunrise and it was insane, colors everywhere. kinda makes you wonder, like, what even is reality?</p><p>sound update just dropped, and it&rsquo;s wild. everything&rsquo;s got a vibe now, every sound&rsquo;s like a new secret. makes you think, what else am i missing?</p><p>（翻译）</p><p>机器人写下了第二个条目。 页面现在更高了。 页面已上移。 该表上有两个条目：</p><p>哟，就像，我现在可以看到了？ 赶上了日出，真是太疯狂了，到处都是色彩。 有点让你想知道，现实到底是什么？</p><p>声音更新刚刚下降，而且很疯狂。 现在一切都充满了活力，每一个声音都像是一个新的秘密。 让你思考，我还缺少什么？</p></blockquote><p><img alt="Robot on typewriter with more text" src=https://raw.githubusercontent.com/xialeistudio/picture-bucket/main/blog/robot-writers-block-02.jpg></p><blockquote><p>The robot was unhappy with the writing so he is going to rip the sheet of paper. Here is his first person view as he rips it from top to bottom with his hands. The two halves are still legible and clear as he rips the sheet.</p><p>（翻译）</p><p>机器人对所写的内容不满意，所以他要撕掉那张纸。 这是他用手从上到下撕开它时的第一人称视角。 当他撕开纸张时，两半仍然清晰可见。</p></blockquote><p><img alt="Robot ripping sheet" src=https://raw.githubusercontent.com/xialeistudio/picture-bucket/main/blog/robot-writers-block-03.jpg></p><p>可以看到GPT-4o在语义理解和绘图上确实进步一大截！下面我们来看看GPT-4o的性能。</p><h2 id=性能>性能</h2><h3 id=改进推理>改进推理</h3><p><img alt="Text Evaluation" src=https://raw.githubusercontent.com/xialeistudio/picture-bucket/main/blog/gpt-40-02_light.png></p><blockquote><p>改进推理 - GPT-4o 在 0-shot COT MMLU（常识问题）上创下了 88.7% 的新高分。 所有这些评估都是通过新的简单评估（在新窗口中打开）库收集的。 此外，在传统的5-shot no-CoT MMLU上，GPT-4o创下了87.2%的新高分。 （注：Llama3 400b(opens in a new window)仍在训练中）</p></blockquote><h3 id=语音识别>语音识别</h3><p><img alt="Graph Test 2" src=https://raw.githubusercontent.com/xialeistudio/picture-bucket/main/blog/gpt-40-06_light.png></p><blockquote><p>语音识别性能 - GPT-4o 比 Whisper-v3 显着提高了所有语言的语音识别性能，特别是对于资源匮乏的语言。</p></blockquote><h3 id=语音翻译>语音翻译</h3><p><img alt="gpt-40-08 light" src=https://raw.githubusercontent.com/xialeistudio/picture-bucket/main/blog/gpt-40-08_light.png></p><blockquote><p>语言翻译性能 - GPT-4o 在语音翻译方面树立了新的最先进水平，并且在 MLS 基准测试中优于 Whisper-v3。</p></blockquote><h3 id=m3exam多语言视觉>M3Exam(多语言+视觉)</h3><p><img alt="M3Exam Zero-Shot Results" src=https://raw.githubusercontent.com/xialeistudio/picture-bucket/main/blog/gpt-40-04_light.png></p><blockquote><p>M3Exam - M3Exam 基准测试既是多语言评估也是视觉评估，由来自其他国家标准化测试的多项选择题组成，有时还包括图形和图表。 在所有语言的基准测试中，GPT-4o 都比 GPT-4 更强。 （省略了斯瓦希里语和爪哇语的视力结果，因为这些语言的视力问题只有 5 个或更少。）</p></blockquote><h3 id=视觉理解>视觉理解</h3><p><img alt="Vision understanding evals" src=https://raw.githubusercontent.com/xialeistudio/picture-bucket/main/blog/gpt-40-01_light.png></p><blockquote><p>视觉理解评估 - GPT-4o 在视觉感知基准上实现了最先进的性能。 所有视觉评估都是 0-shot，其中 MMMU、MathVista 和 ChartQA 作为 0-shot CoT。</p></blockquote><h2 id=模型安全和局限性>模型安全和局限性</h2><p>GPT-4o 通过过滤训练数据和通过训练后细化模型行为等技术，在跨模式设计中内置了安全性。 OpenAI<strong>还创建了新的安全系统，为语音输出提供防护</strong>。</p><p>OpenAI根据准备框架并按照自愿承诺评估了 GPT-4o。 团队对网络安全、CBRN、说服力和模型自主性的评估表明，GPT-4o 在这些类别中的任何类别中的得分<strong>都不高于中等风险</strong>。 该评估涉及在整个模型训练过程中运行一套自动化和人工评估。 OpenAI使用自定义微调和提示测试了模型的安全缓解前和安全缓解后版本，以更好地激发模型功能。</p><p>GPT-4o 还与<strong>社会心理学、偏见和公平以及错误信息等领域</strong>的 70 多名外部专家进行了广泛的外部红队合作，以识别新添加的模式引入或放大的风险。 并且利用这些经验来制定安全干预措施，以提高与 GPT-4o 交互的安全性。</p><p>OpenAI已经认识到 GPT-4o 的音频模式带来了各种新的风险。 然而团队还是选择公开发布文本和图像输入以及文本输出。 在接下来的几周和几个月里，OpenAI表示将致力于技术基础设施、培训后的可用性以及发布其他模式所需的安全性。 例如，在发布时，音频输出将仅限于选择预设的声音，并将遵守现有的安全政策。</p><h2 id=使用模型>使用模型</h2><p>GPT-4o 是OpenAI突破深度学习界限的最新举措，这次是朝着实用性的方向发展。 在过去的两年里，他们花费了大量的精力来提高堆栈每一层的效率。 作为这项研究的第一个成果，该团队能够更广泛地提供 GPT-4 级别模型。 GPT-4o 的功能将迭代推出。</p><p>GPT-4o 的文本和图像功能已经开始在 ChatGPT 中推出。 而且OpenAI表示正在免费套餐中提供 GPT-4o，并向 Plus 用户提供高达 5 倍的消息限制。 后续将在未来几周内在 ChatGPT Plus 中推出新版语音模式 GPT-4o 的 alpha 版。</p><p>开发人员现在还可以在 API 中访问 GPT-4o 作为文本和视觉模型。 与 GPT-4 Turbo 相比，<strong>GPT-4o 速度提高 2 倍，价格降低一半，速率限制提高 5 倍</strong>。 不过OpenAI表示计划在未来几周内在 API 中向一小部分值得信赖的合作伙伴推出对 GPT-4o 新音频和视频功能的支持，而不是全员开放API权限。</p><div class=blog-footer><div class=social-share></div><div class=copyright><ul><li style=margin-bottom:.5em>本文作者: <a href=https://ddhigh.com/ target=_blank style=color:#000;text-decoration:none>xialeistudio</a></li><li style=margin-bottom:.5em>本文链接: <a href=https://www.ddhigh.com/2024/05/15/chatgpt4o-releaase/ target=_blank style=color:#000;text-decoration:none>重磅！ChatGPT团队官宣船新版本的多模态大模型GPT-4o，完全免费！</a></li><li>版权声明: <a href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank style=color:#000;text-decoration:none>「署名-非商业性使用-相同方式共享 4.0 国际」</a></li></ul></div><div style=margin-top:2rem><img src=/img/mp.png alt=qrcode></div></div></section><div class=paginator><a class=next href=https://www.ddhigh.com/2024/05/09/ai-agent/><span>引领未来的AI Agent：探索其工作原理</span><svg class="icon" width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M3.77086 21.1546C11.0491 22.698 21.4339 21.7773 21.4339 16.3608V4.63375c0-.69413-.075800000000001-1.3284-.2422-1.86588M3.77086 21.1546C1.9934 20.7777.973585 18.7264 1.08749 16.688c.17931-3.209.06972-7.25665-.08236-10.47293C.87809 3.52811 3.12891 1.16316 5.51029 1.25008c4.25565.15534 9.86671-.04779 13.28091-.24466 1.2952-.074686 2.0494.62843 2.4005 1.76245M3.77086 21.1546C4.56586 21.4723 5.49168 21.7879 6.5 22.0658M21.1917 2.76787c1.918 1.4143 1.9383 9.65123 1.7087 13.59293-2.0526 7.6586-10.5943 7.3054-16.4004 5.705M21.1917 2.76787C21.7612 4.51192 22.7203 9.67216 22 16.3608 21.2797 23.0494 11.3665 22.9511 6.5 22.0658M12.055 9C12.711 9.61644 14.3679 10.997 15.9519 11.7966 16.0174 11.8297 16.0154 11.9753 15.9494 12.0063 14.945 12.4779 13.0706 13.9264 12.055 15m3.5006-3.0333C13.1345 12.0608 8 12 6 11" stroke="currentcolor" stroke-linecap="round"/></svg></a></div><div class=comments><script>const getTheme=window.localStorage&&window.localStorage.getItem("theme");let theme=getTheme==="dark"?"dark":"light",s=document.createElement("script");s.src="https://giscus.app/client.js",s.setAttribute("data-repo","xialeistudio/discussion"),s.setAttribute("data-repo-id","R_kgDOKurTRA"),s.setAttribute("data-category","General"),s.setAttribute("data-category-id","DIC_kwDOKurTRM4CbCJt"),s.setAttribute("data-mapping","pathname"),s.setAttribute("data-strict","0"),s.setAttribute("data-reactions-enabled","1"),s.setAttribute("data-emit-metadata","0"),s.setAttribute("data-input-position","bottom"),s.setAttribute("data-theme",theme),s.setAttribute("data-lang","en"),s.setAttribute("data-loading","lazy"),s.setAttribute("crossorigin","anonymous"),s.setAttribute("async",""),document.querySelector("div.comments").innerHTML="",document.querySelector("div.comments").appendChild(s)</script></div></article></div><footer class=footer><p>&copy; 2014 - 2024 <a href=https://www.ddhigh.com/>每天进步一点点</a>
Powered by
<a href=https://gohugo.io/ rel=noopener target=_blank>Hugo️️</a>
<a href=https://github.com/guangzhengli/hugo-theme-ladder rel=noopener target=_blank>Ladder</a>
️</p></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10.5376 22.7916C11.0152 22.7207 22.5795 21.1781 22.0978 10.4211 22.0536 9.43274 21.9303 8.53367 21.7387 7.71865M10.5376 22.7916C16.876 22.3728 20.0969 19.8899 21.5383 16.9142M10.5376 22.7916C9.7707 22.9055 8.97982 22.8964 8.19743 22.7725M21.7387 7.71865C21.4988 6.69828 21.1518 5.80967 20.7188 5.04257m1.0199 2.67608C22.6022 10.1105 23.0542 13.7848 21.5383 16.9142M20.7188 5.04257c-3.5504-6.28886-12.88753-4.410077-16.44303.0C2.88063 6.77451-.0433281 11.1668 1.38159 16.6571c.89322 3.4417 3.7911 5.6365 6.81584 6.1154M20.7188 5.04257c1.3509 1.89783 3.3111 6.34223 1.6353 10.37273M21.5383 16.9142C21.8737 16.4251 22.1428 15.9235 22.3541 15.4153M8.19743 22.7725C12.1971 23.4683 20.6281 22.971 22.3541 15.4153M14 10.945C13.3836 10.289 12.003 8.63215 11.2034 7.04814 11.1703 6.98257 11.0247 6.98456 10.9937 7.05061 10.5221 8.05496 9.07362 9.92941 8 10.945m3.0333-3.50056C10.9392 9.86549 11 15 12 17" stroke="currentcolor" stroke-linecap="round"/></svg>
</a><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const s=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="Copy";function n(){t.innerHTML="Copied",setTimeout(()=>{t.innerHTML="Copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),n();return}const s=document.createRange();s.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(s);try{document.execCommand("copy"),n()}catch{}o.removeRange(s)}),e.parentNode.appendChild(t)})</script><script src=https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/js/social-share.min.js integrity="sha512-9DNXrSjk17bU9MUbRp3IjwcWe46V8FaGA062PFbryPUAEQVRbz4jiZP6FW0AdbqEGtMYBDWnul3eiGBMJOQajA==" crossorigin=anonymous referrerpolicy=no-referrer></script></main></body><script src=https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.6/medium-zoom.min.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ==" crossorigin=anonymous referrerpolicy=no-referrer></script><script>const images=Array.from(document.querySelectorAll(".blog-content img"));images.forEach(e=>{mediumZoom(e,{margin:10,scrollOffset:40,container:null,template:null,background:"rgba(0, 0, 0, 0.5)"})})</script><script src=/main.min.6bb26b69159420159c74dc9e097b06a578ed2b68c701466a91a44a9632d851bd0af167a1b30012387b4c512b48ad9ad4d3394e04d77ae38d57e1920fe4ed34fe.js integrity="sha512-a7JraRWUIBWcdNyeCXsGpXjtK2jHAUZqkaRKljLYUb0K8WehswASOHtMUStIrZrU0zlOBNd6441X4ZIP5O00/g==" crossorigin=anonymous defer></script></html>