<!doctype html><html lang=zh><head><meta name=viewport content="width=device-width,initial-scale=1"><title>基于llama3和langchain使用RAG搭建你的私有知识库！</title>
<meta charset=utf-8><meta name=google-adsense-account content="ca-pub-2871082647721658"><meta content="Web开发 ,Java ,Go ,Node.js ,PHP ,Koa ,MySQL ,Redis ,前端 ,后端 ,数据库" name=keywords><meta name=description content="LLM存在时效性和幻觉问题，在 如何用解决大模型时效性和准确性问题？RAG技术核心原理 一文中我介绍了RAG的核心原理，本文将分享如何基于llama3和langchain搭建本地私有知识库。"><meta name=author content="Lei Xia"><link rel=canonical href=https://www.ddhigh.com/2024/04/28/llama3-rag-tutorial/><link rel=alternate type=application/rss+xml href=https://www.ddhigh.com/index.xml title=每天进步一点点><script async src="https://www.googletagmanager.com/gtag/js?id=G-EC3XLVSGKV"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-EC3XLVSGKV")</script><meta property="og:title" content="基于llama3和langchain使用RAG搭建你的私有知识库！"><meta property="og:description" content="LLM存在时效性和幻觉问题，在 如何用解决大模型时效性和准确性问题？RAG技术核心原理 一文中我介绍了RAG的核心原理，本文将分享如何基于llama3和langchain搭建本地私有知识库。"><meta property="og:type" content="article"><meta property="og:url" content="https://www.ddhigh.com/2024/04/28/llama3-rag-tutorial/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-04-28T08:30:57+08:00"><meta property="article:modified_time" content="2024-04-28T08:30:57+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="基于llama3和langchain使用RAG搭建你的私有知识库！"><meta name=twitter:description content="LLM存在时效性和幻觉问题，在 如何用解决大模型时效性和准确性问题？RAG技术核心原理 一文中我介绍了RAG的核心原理，本文将分享如何基于llama3和langchain搭建本地私有知识库。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://www.ddhigh.com/posts/"},{"@type":"ListItem","position":3,"name":"基于llama3和langchain使用RAG搭建你的私有知识库！","item":"https://www.ddhigh.com/2024/04/28/llama3-rag-tutorial/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"基于llama3和langchain使用RAG搭建你的私有知识库！","name":"基于llama3和langchain使用RAG搭建你的私有知识库！","description":"LLM存在时效性和幻觉问题，在 如何用解决大模型时效性和准确性问题？RAG技术核心原理 一文中我介绍了RAG的核心原理，本文将分享如何基于llama3和langchain搭建本地私有知识库。\n","keywords":["llama3","rag"],"articleBody":"LLM存在时效性和幻觉问题，在 如何用解决大模型时效性和准确性问题？RAG技术核心原理 一文中我介绍了RAG的核心原理，本文将分享如何基于llama3和langchain搭建本地私有知识库。\n先决条件 安装ollama和llama3模型，参看 超越GPT-3.5!Llama3个人电脑本地部署教程 安装python3.9 安装langchain用于协调LLM 安装weaviate-client用于向量数据库 pip3 install langchain weaviate-client RAG实践 RAG需要从向量数据库检索上下文然后输入LLM进行生成，因此需要提前将文本数据向量化并存储到向量数据库。主要步骤如下：\n准备文本资料 将文本分块 嵌入以及存储块到向量数据库 新建一个python3项目以及index.py文件，导入需要用到的模块：\nfrom langchain_community.document_loaders import TextLoader # 文本加载器 from langchain.text_splitter import CharacterTextSplitter # 文本分块器 from langchain_community.embeddings import OllamaEmbeddings # Ollama向量嵌入器 import weaviate # 向量数据库 from weaviate.embedded import EmbeddedOptions # 向量嵌入选项 from langchain.prompts import ChatPromptTemplate # 聊天提示模板 from langchain_community.chat_models import ChatOllama # ChatOllma聊天模型 from langchain.schema.runnable import RunnablePassthrough from langchain.schema.output_parser import StrOutputParser # 输出解析器 from langchain_community.vectorstores import Weaviate # 向量数据库 import requests 下载\u0026加载语料 这里使用拜登总统2022年的国情咨文作为示例。文件链接https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/docs/modules/state_of_the_union.txt。langchain提供了多个文档加载器，这里我们使用TextLoaders即可。\n# 下载文件 url = \"https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/docs/modules/state_of_the_union.txt\" res = requests.get(url) with open(\"state_of_the_union.txt\", \"w\") as f: f.write(res.text) # 加载文件 loader = TextLoader('./state_of_the_union.txt') documents = loader.load() 语料分块 由于原始文档过大，超出了LLM的上下文窗口，需要将其分块才能让LLM识别。LangChain 提供了许多内置的文本分块工具，这里用CharacterTextSplitter作为示例：\ntext_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50) chunks = text_splitter.split_documents(documents) 嵌入以及存储到向量数据库 为了对语料分块进行搜索，需要为每个块生成向量并嵌入文档，最后将文档和向量一起存储。这里使用Ollama\u0026llama3生成向量，并存储到Weaviate向量数据库。\nclient = weaviate.Client( embedded_options=EmbeddedOptions() ) print(\"store vector\") vectorstore = Weaviate.from_documents( client=client, documents=chunks, embedding=OllamaEmbeddings(model=\"llama3\"), by_text=False ) 检索 \u0026 增强 向量数据库加载数据后，可以作为检索器，通过用户查询和嵌入向量之间的语义相似性获取数据，然后使用一个固定的聊天模板即可。\n# 检索器 retriever = vectorstore.as_retriever() # LLM提示模板 template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise. Question: {question} Context: {context} Answer: \"\"\" prompt = ChatPromptTemplate.from_template(template) 生成 最后，将检索器、聊天模板以及LLM组合成RAG链就可以了。\nllm = ChatOllama(model=\"llama3\", temperature=10) rag_chain = ( {\"context\": retriever, \"question\": RunnablePassthrough()} # 上下文信息 | prompt | llm | StrOutputParser() ) # 开始查询\u0026生成 query = \"What did the president mainly say?\" print(rag_chain.invoke(query)) 上面的示例中我问了LLM总统主要说了什么，LLM回答如下：\nThe president mainly talked about continuing efforts to combat COVID-19, including vaccination rates and measures to prepare for new variants. They also discussed investments in workers, communities, and law enforcement, with a focus on fairness and justice. The tone was hopeful and emphasized the importance of taking action to improve Americans' lives. 可以看到还是像那么回事的，LLM使用的输入预料的内容答复了一些关于新冠疫情以及工作、社区等内容。\nlangchain支持多种LLM，有需要的读者可以尝试下使用OpenAI提供的LLM。\n读者可以根据需要替换下输入预料，构造自己的私有知识检索库。\n本文所有代码如下：\nfrom langchain_community.document_loaders import TextLoader from langchain.text_splitter import CharacterTextSplitter from langchain_community.embeddings import OllamaEmbeddings import weaviate from weaviate.embedded import EmbeddedOptions from langchain.prompts import ChatPromptTemplate from langchain_community.chat_models import ChatOllama from langchain.schema.runnable import RunnablePassthrough from langchain.schema.output_parser import StrOutputParser from langchain_community.vectorstores import Weaviate import requests # 下载数据 url = \"https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/docs/modules/state_of_the_union.txt\" res = requests.get(url) with open(\"state_of_the_union.txt\", \"w\") as f: f.write(res.text) # 加载数据 loader = TextLoader('./state_of_the_union.txt') documents = loader.load() # 文本分块 text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50) chunks = text_splitter.split_documents(documents) # 初始化向量数据库并嵌入目标文档 client = weaviate.Client( embedded_options=EmbeddedOptions() ) vectorstore = Weaviate.from_documents( client=client, documents=chunks, embedding=OllamaEmbeddings(model=\"llama3\"), by_text=False ) # 检索器 retriever = vectorstore.as_retriever() # LLM提示模板 template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise. Question: {question} Context: {context} Answer: \"\"\" prompt = ChatPromptTemplate.from_template(template) llm = ChatOllama(model=\"llama3\", temperature=10) rag_chain = ( {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser() ) # 开始查询\u0026生成 query = \"What did the president mainly say?\" print(rag_chain.invoke(query)) ","wordCount":"451","inLanguage":"zh","datePublished":"2024-04-28T08:30:57+08:00","dateModified":"2024-04-28T08:30:57+08:00","author":{"@type":"Person","name":"Lei Xia"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.ddhigh.com/2024/04/28/llama3-rag-tutorial/"},"publisher":{"@type":"Organization","name":"每天进步一点点","logo":{"@type":"ImageObject","url":"https://www.ddhigh.com/favicon.ico"}}}</script><link rel=icon href=/img/favicon.ico sizes=16x16><link rel=apple-touch-icon href=/img/favicon.ico><link rel=manifest href=/img/favicon.ico><link href=/titilliumweb/titilliumweb.css rel=stylesheet><link rel=stylesheet href=/css/main.min.0eb4160ba4a2d63122fe8ae83f1560951a87ab510d5dab0615973b5206555759.css integrity="sha256-DrQWC6Si1jEi/oroPxVglRqHq1ENXasGFZc7UgZVV1k=" crossorigin=anonymous media=screen><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/css/share.min.css integrity="sha512-ygEyjMC6rqnzJqWGjRTJUPYMEs9JUOm3i7OWUS9CgQ4XkBUvMsgCS1I8JqavidQ2ClHcREB7IbA2mN08+r9Elg==" crossorigin=anonymous referrerpolicy=no-referrer><link rel=stylesheet href=/scss/highlight/github-dark.min.min.66034289ee9a113219a2c4aae0a8bd2095ab255c832a42efcf5863f10814e7a1.css><script src=/js/highlight.min.min.894ca9c68afab956438c4926a0dc7f5293e04e08595bd27abdb123e94801f684.js></script><script>hljs.highlightAll()</script><script>$darkModeInit.Content|safeJS</script><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2871082647721658" crossorigin=anonymous></script></head><body><main class=wrapper><nav class=navigation><section class=container><a class=navigation-brand href=/>每天进步一点点
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><span></span><span></span><span></span></label><ul class=navigation-list id=navigation-list><li class="navigation-item navigation-menu"><a class=navigation-link href=/>首页</a></li><li class="navigation-item navigation-menu"><a class=navigation-link href=/archives>归档</a></li><li class="navigation-item navigation-menu"><a class=navigation-link href=/books>出版物</a></li><li class="navigation-item navigation-menu"><a class=navigation-link href=/guestbook>留言板</a></li><li class="navigation-item menu-separator"><span>|</span></li><li class="navigation-item navigation-social"><a class=navigation-link href=https://github.com/xialeistudio><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-github"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a></li><li class="navigation-item navigation-dark"><button id=mode type=button aria-label="toggle user light or dark theme">
<span class=toggle-dark><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-moon"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg></span><span class=toggle-light><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-sun"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></span></button></li><li class="navigation-item navigation-language"><a href=https://www.ddhigh.com/en/2024/04/28/llama3-rag-tutorial/>EN</a></li></ul></section></nav><div id=content><article class=blog-single><header class=blog-title><h1>基于llama3和langchain使用RAG搭建你的私有知识库！</h1></header><p><small>2024年4月28日&nbsp;· 451 字&nbsp;· 3 分钟</small><p><div class=blog-toc><nav id=TableOfContents><ul><li><a href=#先决条件>先决条件</a></li><li><a href=#rag实践>RAG实践</a><ul><li><a href=#下载加载语料>下载&加载语料</a></li><li><a href=#语料分块>语料分块</a></li><li><a href=#嵌入以及存储到向量数据库>嵌入以及存储到向量数据库</a></li><li><a href=#检索--增强>检索 & 增强</a></li><li><a href=#生成>生成</a></li></ul></li></ul></nav></div><section class=blog-content><p>LLM存在时效性和幻觉问题，在 <a href=https://www.ddhigh.com/2024/04/24/retrieval-augmented-generation/>如何用解决大模型时效性和准确性问题？RAG技术核心原理</a> 一文中我介绍了RAG的核心原理，本文将分享如何基于llama3和langchain搭建本地私有知识库。</p><h2 id=先决条件>先决条件</h2><ul><li>安装ollama和llama3模型，参看 <a href=https://www.ddhigh.com/2024/04/20/%E8%B6%85%E8%B6%8Agpt-3.5llama3%E4%B8%AA%E4%BA%BA%E7%94%B5%E8%84%91%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E6%95%99%E7%A8%8B/>超越GPT-3.5!Llama3个人电脑本地部署教程</a></li><li>安装python3.9</li><li>安装langchain用于协调LLM</li><li>安装weaviate-client用于向量数据库</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip3 install langchain weaviate-client
</span></span></code></pre></div><h2 id=rag实践>RAG实践</h2><p>RAG需要从向量数据库检索上下文然后输入LLM进行生成，因此需要提前将文本数据向量化并存储到向量数据库。主要步骤如下：</p><ol><li>准备文本资料</li><li>将文本分块</li><li>嵌入以及存储块到向量数据库</li></ol><p>新建一个python3项目以及<code>index.py</code>文件，导入需要用到的模块：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> langchain_community.document_loaders <span style=color:#f92672>import</span> TextLoader <span style=color:#75715e># 文本加载器</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain.text_splitter <span style=color:#f92672>import</span> CharacterTextSplitter <span style=color:#75715e># 文本分块器</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain_community.embeddings <span style=color:#f92672>import</span> OllamaEmbeddings <span style=color:#75715e># Ollama向量嵌入器</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> weaviate <span style=color:#75715e># 向量数据库</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> weaviate.embedded <span style=color:#f92672>import</span> EmbeddedOptions <span style=color:#75715e># 向量嵌入选项</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain.prompts <span style=color:#f92672>import</span> ChatPromptTemplate <span style=color:#75715e># 聊天提示模板</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain_community.chat_models <span style=color:#f92672>import</span> ChatOllama <span style=color:#75715e># ChatOllma聊天模型</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain.schema.runnable <span style=color:#f92672>import</span> RunnablePassthrough
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain.schema.output_parser <span style=color:#f92672>import</span> StrOutputParser <span style=color:#75715e># 输出解析器</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain_community.vectorstores <span style=color:#f92672>import</span> Weaviate <span style=color:#75715e># 向量数据库</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> requests
</span></span></code></pre></div><h3 id=下载加载语料>下载&加载语料</h3><p>这里使用拜登总统2022年的国情咨文作为示例。文件链接https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/docs/modules/state_of_the_union.txt。langchain提供了多个文档加载器，这里我们使用<code>TextLoaders</code>即可。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># 下载文件</span>
</span></span><span style=display:flex><span>url <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/docs/modules/state_of_the_union.txt&#34;</span>
</span></span><span style=display:flex><span>res <span style=color:#f92672>=</span> requests<span style=color:#f92672>.</span>get(url)
</span></span><span style=display:flex><span><span style=color:#66d9ef>with</span> open(<span style=color:#e6db74>&#34;state_of_the_union.txt&#34;</span>, <span style=color:#e6db74>&#34;w&#34;</span>) <span style=color:#66d9ef>as</span> f:
</span></span><span style=display:flex><span>    f<span style=color:#f92672>.</span>write(res<span style=color:#f92672>.</span>text)
</span></span><span style=display:flex><span><span style=color:#75715e># 加载文件</span>
</span></span><span style=display:flex><span>loader <span style=color:#f92672>=</span> TextLoader(<span style=color:#e6db74>&#39;./state_of_the_union.txt&#39;</span>)
</span></span><span style=display:flex><span>documents <span style=color:#f92672>=</span> loader<span style=color:#f92672>.</span>load()
</span></span></code></pre></div><h3 id=语料分块>语料分块</h3><p>由于原始文档过大，超出了LLM的上下文窗口，需要将其分块才能让LLM识别。LangChain 提供了许多内置的文本分块工具，这里用<code>CharacterTextSplitter</code>作为示例：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>text_splitter <span style=color:#f92672>=</span> CharacterTextSplitter(chunk_size<span style=color:#f92672>=</span><span style=color:#ae81ff>500</span>, chunk_overlap<span style=color:#f92672>=</span><span style=color:#ae81ff>50</span>)
</span></span><span style=display:flex><span>chunks <span style=color:#f92672>=</span> text_splitter<span style=color:#f92672>.</span>split_documents(documents)
</span></span></code></pre></div><h3 id=嵌入以及存储到向量数据库>嵌入以及存储到向量数据库</h3><p>为了对语料分块进行搜索，需要为每个块生成向量并嵌入文档，最后将文档和向量一起存储。这里使用Ollama&amp;llama3生成向量，并存储到Weaviate向量数据库。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>client <span style=color:#f92672>=</span> weaviate<span style=color:#f92672>.</span>Client(
</span></span><span style=display:flex><span>    embedded_options<span style=color:#f92672>=</span>EmbeddedOptions()
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;store vector&#34;</span>)
</span></span><span style=display:flex><span>vectorstore <span style=color:#f92672>=</span> Weaviate<span style=color:#f92672>.</span>from_documents(
</span></span><span style=display:flex><span>    client<span style=color:#f92672>=</span>client,
</span></span><span style=display:flex><span>    documents<span style=color:#f92672>=</span>chunks,
</span></span><span style=display:flex><span>    embedding<span style=color:#f92672>=</span>OllamaEmbeddings(model<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;llama3&#34;</span>),
</span></span><span style=display:flex><span>    by_text<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><h3 id=检索--增强>检索 & 增强</h3><p>向量数据库加载数据后，可以作为检索器，通过用户查询和嵌入向量之间的语义相似性获取数据，然后使用一个固定的聊天模板即可。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># 检索器</span>
</span></span><span style=display:flex><span>retriever <span style=color:#f92672>=</span> vectorstore<span style=color:#f92672>.</span>as_retriever()
</span></span><span style=display:flex><span><span style=color:#75715e># LLM提示模板</span>
</span></span><span style=display:flex><span>template <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;&#34;You are an assistant for question-answering tasks. 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   Use the following pieces of retrieved context to answer the question. 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   If you don&#39;t know the answer, just say that you don&#39;t know. 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   Use three sentences maximum and keep the answer concise.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   Question: </span><span style=color:#e6db74>{question}</span><span style=color:#e6db74> 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   Context: </span><span style=color:#e6db74>{context}</span><span style=color:#e6db74> 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   Answer:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>prompt <span style=color:#f92672>=</span> ChatPromptTemplate<span style=color:#f92672>.</span>from_template(template)
</span></span></code></pre></div><h3 id=生成>生成</h3><p>最后，将检索器、聊天模板以及LLM组合成RAG链就可以了。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>llm <span style=color:#f92672>=</span> ChatOllama(model<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;llama3&#34;</span>, temperature<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>)
</span></span><span style=display:flex><span>rag_chain <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>        {<span style=color:#e6db74>&#34;context&#34;</span>: retriever, <span style=color:#e6db74>&#34;question&#34;</span>: RunnablePassthrough()} <span style=color:#75715e># 上下文信息</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>|</span> prompt
</span></span><span style=display:flex><span>        <span style=color:#f92672>|</span> llm
</span></span><span style=display:flex><span>        <span style=color:#f92672>|</span> StrOutputParser()
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span><span style=color:#75715e># 开始查询&amp;生成</span>
</span></span><span style=display:flex><span>query <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;What did the president mainly say?&#34;</span>
</span></span><span style=display:flex><span>print(rag_chain<span style=color:#f92672>.</span>invoke(query))
</span></span></code></pre></div><p>上面的示例中我问了LLM总统主要说了什么，LLM回答如下：</p><pre tabindex=0><code>The president mainly talked about continuing efforts to combat COVID-19, including vaccination rates and measures to prepare for new variants. They also discussed investments in workers, communities, and law enforcement, with a focus on fairness and justice. The tone was hopeful and emphasized the importance of taking action to improve Americans&#39; lives.
</code></pre><p>可以看到还是像那么回事的，LLM使用的输入预料的内容答复了一些关于新冠疫情以及工作、社区等内容。</p><blockquote><p>langchain支持多种LLM，有需要的读者可以尝试下使用OpenAI提供的LLM。</p></blockquote><p>读者可以根据需要替换下输入预料，构造自己的私有知识检索库。</p><p>本文所有代码如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> langchain_community.document_loaders <span style=color:#f92672>import</span> TextLoader
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain.text_splitter <span style=color:#f92672>import</span> CharacterTextSplitter
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain_community.embeddings <span style=color:#f92672>import</span> OllamaEmbeddings
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> weaviate
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> weaviate.embedded <span style=color:#f92672>import</span> EmbeddedOptions
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain.prompts <span style=color:#f92672>import</span> ChatPromptTemplate
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain_community.chat_models <span style=color:#f92672>import</span> ChatOllama
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain.schema.runnable <span style=color:#f92672>import</span> RunnablePassthrough
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain.schema.output_parser <span style=color:#f92672>import</span> StrOutputParser
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain_community.vectorstores <span style=color:#f92672>import</span> Weaviate
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> requests
</span></span><span style=display:flex><span><span style=color:#75715e># 下载数据</span>
</span></span><span style=display:flex><span>url <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/docs/modules/state_of_the_union.txt&#34;</span>
</span></span><span style=display:flex><span>res <span style=color:#f92672>=</span> requests<span style=color:#f92672>.</span>get(url)
</span></span><span style=display:flex><span><span style=color:#66d9ef>with</span> open(<span style=color:#e6db74>&#34;state_of_the_union.txt&#34;</span>, <span style=color:#e6db74>&#34;w&#34;</span>) <span style=color:#66d9ef>as</span> f:
</span></span><span style=display:flex><span>    f<span style=color:#f92672>.</span>write(res<span style=color:#f92672>.</span>text)
</span></span><span style=display:flex><span><span style=color:#75715e># 加载数据</span>
</span></span><span style=display:flex><span>loader <span style=color:#f92672>=</span> TextLoader(<span style=color:#e6db74>&#39;./state_of_the_union.txt&#39;</span>)
</span></span><span style=display:flex><span>documents <span style=color:#f92672>=</span> loader<span style=color:#f92672>.</span>load()
</span></span><span style=display:flex><span><span style=color:#75715e># 文本分块</span>
</span></span><span style=display:flex><span>text_splitter <span style=color:#f92672>=</span> CharacterTextSplitter(chunk_size<span style=color:#f92672>=</span><span style=color:#ae81ff>500</span>, chunk_overlap<span style=color:#f92672>=</span><span style=color:#ae81ff>50</span>)
</span></span><span style=display:flex><span>chunks <span style=color:#f92672>=</span> text_splitter<span style=color:#f92672>.</span>split_documents(documents)
</span></span><span style=display:flex><span><span style=color:#75715e># 初始化向量数据库并嵌入目标文档</span>
</span></span><span style=display:flex><span>client <span style=color:#f92672>=</span> weaviate<span style=color:#f92672>.</span>Client(
</span></span><span style=display:flex><span>    embedded_options<span style=color:#f92672>=</span>EmbeddedOptions()
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>vectorstore <span style=color:#f92672>=</span> Weaviate<span style=color:#f92672>.</span>from_documents(
</span></span><span style=display:flex><span>    client<span style=color:#f92672>=</span>client,
</span></span><span style=display:flex><span>    documents<span style=color:#f92672>=</span>chunks,
</span></span><span style=display:flex><span>    embedding<span style=color:#f92672>=</span>OllamaEmbeddings(model<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;llama3&#34;</span>),
</span></span><span style=display:flex><span>    by_text<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span><span style=color:#75715e># 检索器</span>
</span></span><span style=display:flex><span>retriever <span style=color:#f92672>=</span> vectorstore<span style=color:#f92672>.</span>as_retriever()
</span></span><span style=display:flex><span><span style=color:#75715e># LLM提示模板</span>
</span></span><span style=display:flex><span>template <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;&#34;You are an assistant for question-answering tasks. 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   Use the following pieces of retrieved context to answer the question. 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   If you don&#39;t know the answer, just say that you don&#39;t know. 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   Use three sentences maximum and keep the answer concise.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   Question: </span><span style=color:#e6db74>{question}</span><span style=color:#e6db74> 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   Context: </span><span style=color:#e6db74>{context}</span><span style=color:#e6db74> 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   Answer:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>prompt <span style=color:#f92672>=</span> ChatPromptTemplate<span style=color:#f92672>.</span>from_template(template)
</span></span><span style=display:flex><span>llm <span style=color:#f92672>=</span> ChatOllama(model<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;llama3&#34;</span>, temperature<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>)
</span></span><span style=display:flex><span>rag_chain <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>        {<span style=color:#e6db74>&#34;context&#34;</span>: retriever, <span style=color:#e6db74>&#34;question&#34;</span>: RunnablePassthrough()}
</span></span><span style=display:flex><span>        <span style=color:#f92672>|</span> prompt
</span></span><span style=display:flex><span>        <span style=color:#f92672>|</span> llm
</span></span><span style=display:flex><span>        <span style=color:#f92672>|</span> StrOutputParser()
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span><span style=color:#75715e># 开始查询&amp;生成</span>
</span></span><span style=display:flex><span>query <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;What did the president mainly say?&#34;</span>
</span></span><span style=display:flex><span>print(rag_chain<span style=color:#f92672>.</span>invoke(query))
</span></span></code></pre></div><div class=blog-footer><div class=social-share></div><div class=copyright><ul><li style=margin-bottom:.5em>本文作者: <a href=https://ddhigh.com/ target=_blank style=color:#000;text-decoration:none>xialeistudio</a></li><li style=margin-bottom:.5em>本文链接: <a href=https://www.ddhigh.com/2024/04/28/llama3-rag-tutorial/ target=_blank style=color:#000;text-decoration:none>基于llama3和langchain使用RAG搭建你的私有知识库！</a></li><li>版权声明: <a href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank style=color:#000;text-decoration:none>「署名-非商业性使用-相同方式共享 4.0 国际」</a></li></ul></div><div style=margin-top:2rem><img src=/img/mp.png alt=qrcode></div></div></section><div class=paginator><a class=prev href=https://www.ddhigh.com/2024/04/30/free-gpt4/><svg class="icon" width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M3.77086 21.1546C11.0491 22.698 21.4339 21.7773 21.4339 16.3608V4.63375c0-.69413-.075800000000001-1.3284-.2422-1.86588M3.77086 21.1546C1.9934 20.7777.973585 18.7264 1.08749 16.688c.17931-3.209.06972-7.25665-.08236-10.47293C.87809 3.52811 3.12891 1.16316 5.51029 1.25008c4.25565.15534 9.86671-.04779 13.28091-.24466 1.2952-.074686 2.0494.62843 2.4005 1.76245M3.77086 21.1546C4.56586 21.4723 5.49168 21.7879 6.5 22.0658M21.1917 2.76787c1.918 1.4143 1.9383 9.65123 1.7087 13.59293-2.0526 7.6586-10.5943 7.3054-16.4004 5.705M21.1917 2.76787C21.7612 4.51192 22.7203 9.67216 22 16.3608 21.2797 23.0494 11.3665 22.9511 6.5 22.0658M9.94496 9C9.28897 9.61644 7.63215 10.997 6.04814 11.7966 5.98257 11.8297 5.98456 11.9753 6.05061 12.0063c1.00435.4716 2.8788 1.9201 3.89435 2.9937M6.44444 11.9667C8.86549 12.0608 14 12 16 11" stroke="currentcolor" stroke-linecap="round"/></svg><span>白嫖GPT4.0！</span></a>
<a class=next href=https://www.ddhigh.com/2024/04/25/setup-local-codegpt-with-llama3/><span>拥有你自己的Copilot!基于Llama3和CodeGPT部署本地Copilot，断网也能使用！</span><svg class="icon" width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M3.77086 21.1546C11.0491 22.698 21.4339 21.7773 21.4339 16.3608V4.63375c0-.69413-.075800000000001-1.3284-.2422-1.86588M3.77086 21.1546C1.9934 20.7777.973585 18.7264 1.08749 16.688c.17931-3.209.06972-7.25665-.08236-10.47293C.87809 3.52811 3.12891 1.16316 5.51029 1.25008c4.25565.15534 9.86671-.04779 13.28091-.24466 1.2952-.074686 2.0494.62843 2.4005 1.76245M3.77086 21.1546C4.56586 21.4723 5.49168 21.7879 6.5 22.0658M21.1917 2.76787c1.918 1.4143 1.9383 9.65123 1.7087 13.59293-2.0526 7.6586-10.5943 7.3054-16.4004 5.705M21.1917 2.76787C21.7612 4.51192 22.7203 9.67216 22 16.3608 21.2797 23.0494 11.3665 22.9511 6.5 22.0658M12.055 9C12.711 9.61644 14.3679 10.997 15.9519 11.7966 16.0174 11.8297 16.0154 11.9753 15.9494 12.0063 14.945 12.4779 13.0706 13.9264 12.055 15m3.5006-3.0333C13.1345 12.0608 8 12 6 11" stroke="currentcolor" stroke-linecap="round"/></svg></a></div><div class=comments><script>const getTheme=window.localStorage&&window.localStorage.getItem("theme");let theme=getTheme==="dark"?"dark":"light",s=document.createElement("script");s.src="https://giscus.app/client.js",s.setAttribute("data-repo","xialeistudio/discussion"),s.setAttribute("data-repo-id","R_kgDOKurTRA"),s.setAttribute("data-category","General"),s.setAttribute("data-category-id","DIC_kwDOKurTRM4CbCJt"),s.setAttribute("data-mapping","pathname"),s.setAttribute("data-strict","0"),s.setAttribute("data-reactions-enabled","1"),s.setAttribute("data-emit-metadata","0"),s.setAttribute("data-input-position","bottom"),s.setAttribute("data-theme",theme),s.setAttribute("data-lang","en"),s.setAttribute("data-loading","lazy"),s.setAttribute("crossorigin","anonymous"),s.setAttribute("async",""),document.querySelector("div.comments").innerHTML="",document.querySelector("div.comments").appendChild(s)</script></div></article></div><footer class=footer><p>&copy; 2014 - 2024 <a href=https://www.ddhigh.com>每天进步一点点</a>
Powered by
<a href=https://gohugo.io/ rel=noopener target=_blank>Hugo️️</a>
<a href=https://github.com/guangzhengli/hugo-theme-ladder rel=noopener target=_blank>Ladder</a>
️</p></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10.5376 22.7916C11.0152 22.7207 22.5795 21.1781 22.0978 10.4211 22.0536 9.43274 21.9303 8.53367 21.7387 7.71865M10.5376 22.7916C16.876 22.3728 20.0969 19.8899 21.5383 16.9142M10.5376 22.7916C9.7707 22.9055 8.97982 22.8964 8.19743 22.7725M21.7387 7.71865C21.4988 6.69828 21.1518 5.80967 20.7188 5.04257m1.0199 2.67608C22.6022 10.1105 23.0542 13.7848 21.5383 16.9142M20.7188 5.04257c-3.5504-6.28886-12.88753-4.410077-16.44303.0C2.88063 6.77451-.0433281 11.1668 1.38159 16.6571c.89322 3.4417 3.7911 5.6365 6.81584 6.1154M20.7188 5.04257c1.3509 1.89783 3.3111 6.34223 1.6353 10.37273M21.5383 16.9142C21.8737 16.4251 22.1428 15.9235 22.3541 15.4153M8.19743 22.7725C12.1971 23.4683 20.6281 22.971 22.3541 15.4153M14 10.945C13.3836 10.289 12.003 8.63215 11.2034 7.04814 11.1703 6.98257 11.0247 6.98456 10.9937 7.05061 10.5221 8.05496 9.07362 9.92941 8 10.945m3.0333-3.50056C10.9392 9.86549 11 15 12 17" stroke="currentcolor" stroke-linecap="round"/></svg></a><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const s=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="Copy";function n(){t.innerHTML="Copied",setTimeout(()=>{t.innerHTML="Copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),n();return}const s=document.createRange();s.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(s);try{document.execCommand("copy"),n()}catch{}o.removeRange(s)}),e.parentNode.appendChild(t)})</script><script src=https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/js/social-share.min.js integrity="sha512-9DNXrSjk17bU9MUbRp3IjwcWe46V8FaGA062PFbryPUAEQVRbz4jiZP6FW0AdbqEGtMYBDWnul3eiGBMJOQajA==" crossorigin=anonymous referrerpolicy=no-referrer></script></main></body><script src=https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.6/medium-zoom.min.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ==" crossorigin=anonymous referrerpolicy=no-referrer></script><script>const images=Array.from(document.querySelectorAll(".blog-content img"));images.forEach(e=>{mediumZoom(e,{margin:10,scrollOffset:40,container:null,template:null,background:"rgba(0, 0, 0, 0.5)"})})</script><script src=/main.min.6bb26b69159420159c74dc9e097b06a578ed2b68c701466a91a44a9632d851bd0af167a1b30012387b4c512b48ad9ad4d3394e04d77ae38d57e1920fe4ed34fe.js integrity="sha512-a7JraRWUIBWcdNyeCXsGpXjtK2jHAUZqkaRKljLYUb0K8WehswASOHtMUStIrZrU0zlOBNd6441X4ZIP5O00/g==" crossorigin=anonymous defer></script></html>