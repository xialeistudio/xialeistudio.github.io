<!doctype html><html lang=zh><head><meta name=viewport content="width=device-width,initial-scale=1"><title>Meta官宣Llama3：迄今为止最强大的开源大模型</title>
<meta charset=utf-8><meta name=google-adsense-account content="ca-pub-2871082647721658"><meta content="Web开发 ,Java ,Go ,Node.js ,PHP ,Koa ,MySQL ,Redis ,前端 ,后端 ,数据库" name=keywords><meta name=description content="4月18日，Meta在官方博客官宣了Llama3，标志着人工智能领域迈向了一个重要的飞跃。此版本具有经过预训练和指令微调的语言模型，具有 8B(80亿) 和 70B(700亿) 参数，可以支持广泛的用例。 Llama3在各种行业基准上展示了最先进的性能，并提供了新功能，包括改进的推理能力。"><meta name=author content="Lei Xia"><link rel=canonical href=https://www.ddhigh.com/2024/04/19/meta%E5%AE%98%E5%AE%A3llama3%E8%BF%84%E4%BB%8A%E4%B8%BA%E6%AD%A2%E6%9C%80%E5%BC%BA%E5%A4%A7%E7%9A%84%E5%BC%80%E6%BA%90%E5%A4%A7%E6%A8%A1%E5%9E%8B/><link rel=alternate type=application/rss+xml href=https://www.ddhigh.com//index.xml title=每天进步一点点><script async defer data-website-id=52f8f0f9-d93d-466b-8ef5-508aae8c4ed4 src=https://analysis.ddhigh.com/script.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-EC3XLVSGKV"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-EC3XLVSGKV")</script><meta property="og:url" content="https://www.ddhigh.com/2024/04/19/meta%E5%AE%98%E5%AE%A3llama3%E8%BF%84%E4%BB%8A%E4%B8%BA%E6%AD%A2%E6%9C%80%E5%BC%BA%E5%A4%A7%E7%9A%84%E5%BC%80%E6%BA%90%E5%A4%A7%E6%A8%A1%E5%9E%8B/"><meta property="og:site_name" content="每天进步一点点"><meta property="og:title" content="Meta官宣Llama3：迄今为止最强大的开源大模型"><meta property="og:description" content="4月18日，Meta在官方博客官宣了Llama3，标志着人工智能领域迈向了一个重要的飞跃。此版本具有经过预训练和指令微调的语言模型，具有 8B(80亿) 和 70B(700亿) 参数，可以支持广泛的用例。 Llama3在各种行业基准上展示了最先进的性能，并提供了新功能，包括改进的推理能力。"><meta property="og:locale" content="zh"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-04-19T17:30:57+08:00"><meta property="article:modified_time" content="2024-04-19T17:30:57+08:00"><meta property="article:tag" content="Llama3"><meta name=twitter:card content="summary"><meta name=twitter:title content="Meta官宣Llama3：迄今为止最强大的开源大模型"><meta name=twitter:description content="4月18日，Meta在官方博客官宣了Llama3，标志着人工智能领域迈向了一个重要的飞跃。此版本具有经过预训练和指令微调的语言模型，具有 8B(80亿) 和 70B(700亿) 参数，可以支持广泛的用例。 Llama3在各种行业基准上展示了最先进的性能，并提供了新功能，包括改进的推理能力。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://www.ddhigh.com/posts/"},{"@type":"ListItem","position":2,"name":"Meta官宣Llama3：迄今为止最强大的开源大模型","item":"https://www.ddhigh.com/2024/04/19/meta%E5%AE%98%E5%AE%A3llama3%E8%BF%84%E4%BB%8A%E4%B8%BA%E6%AD%A2%E6%9C%80%E5%BC%BA%E5%A4%A7%E7%9A%84%E5%BC%80%E6%BA%90%E5%A4%A7%E6%A8%A1%E5%9E%8B/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Meta官宣Llama3：迄今为止最强大的开源大模型","name":"Meta官宣Llama3：迄今为止最强大的开源大模型","description":"4月18日，Meta在官方博客官宣了Llama3，标志着人工智能领域迈向了一个重要的飞跃。此版本具有经过预训练和指令微调的语言模型，具有 8B(80亿) 和 70B(700亿) 参数，可以支持广泛的用例。 Llama3在各种行业基准上展示了最先进的性能，并提供了新功能，包括改进的推理能力。","keywords":["llama3"],"articleBody":"4月18日，Meta在官方博客官宣了Llama3，标志着人工智能领域迈向了一个重要的飞跃。此版本具有经过预训练和指令微调的语言模型，具有 8B(80亿) 和 70B(700亿) 参数，可以支持广泛的用例。 Llama3在各种行业基准上展示了最先进的性能，并提供了新功能，包括改进的推理能力。\n领先的性能 新的 8B 和 70B 参数 Llama 3 模型是 Llama 2 模型的重大飞跃，为这些规模的 LLM 模型确立了新的先进水平。得益于预训练和后训练的改进，Llama3的预训练和指令微调模型是目前 8B 和 70B 参数规模下最好的模型。后期训练程序的改进大大降低了错误拒绝率，提高了对齐度，并增加了模型响应的多样性。其次，推理、代码生成和指令跟踪等能力也有了很大提高，这使得 Llama 3 的可操控性更强。\n在开发 Llama 3 的过程中，Meta为了优化其在真实世界场景中的性能开发了一个新的高质量人类评估集。该评估集包含 1,800 个提示，涵盖 12 种关键用例：征求建议、头脑风暴、分类、封闭式问题解答、编码、创意写作、提取、角色/人物角色、开放式问题解答、推理、改写和总结。\n有意思的是，为了防止模型在这个评估集上意外地过度拟合，即使是Meta自己的建模团队也无法访问这个评估集。下图显示了针对 Claude Sonnet、Mistral Medium 和 GPT-3.5 对这些类别和提示进行的人工评估的汇总结果。\n在与Cloud Sonnet、Mistral Medium、GPT-3.5的对比中，Llama3表现更好。\n三大关键设计理念 Meta在博客中表示，Llama 3 选择了一个相对标准的Transformer架构。与 Llama 2 相比，做了几项关键改进。Llama 3 使用了一个 128K token的tokenizer，它能更有效地编码语言，从而大幅提高模型性能。为了提高 Llama 3 模型的推理效率，Meta在 8B 和 70B 大小的模型中都采用了分组查询关注 (Grouped Query Attention, GQA)。并且在 8,192 个token的序列上对模型进行了训练，并使用掩码来确保自我关注不会跨越文档边界。\n训练数据 为了训练最佳的语言模型，筛选大规模、高质量的训练数据至关重要。根据设计原则，Meta在预训练数据上进行了大量的投入。Llama 3在超过1500亿token的数据上进行了预训练，这些数据均来自公开可得的来源。Llama3的训练数据集比Llama 2使用的数据集大七倍，并且包含了四倍的代码。为了应对即将到来的多语言应用场景，Llama 3的预训练数据集中有超过5%的高质量非英语数据，涵盖了30多种语言。\n为确保Llama 3的训练数据具有最高的质量，Meta开发了一系列的数据过滤流程，包括使用启发式过滤器、NSFW过滤器、语义去重方法和文本分类器来预测数据质量。Meta在博客中表示，之前的Llama版本在识别高质量数据方面表现出人意料的良好，因此他们使用Llama 2生成了训练数据，用于为驱动Llama 3的文本质量分类器提供支持。\n此外，Meta还进行了大量实验使他们能够选择一种数据混合方式，以确保Llama 3在各种用例中都能表现出色，包括问答、科学技术工程数学（STEM）、编码、历史知识等。\n扩大训练规模 为了充分利用Llama 3模型的预训练数据，Meta致力于扩展预训练过程，并采取了多项关键措施。首先，他们开发了详细的scaling laws，通过预测最大模型在关键任务上的性能，Meta能够在实际训练之前做出明智的决策。\n其次，Meta在Llama 3的开发过程中观察到了有关模型训练规模的新行为。Meta发现，即使在训练了两个数量级更多的数据后，模型的性能仍然持续改善，使他们能够构建更强大、更高效的语言模型。\n为了训练最大规模的Llama 3模型，Meta采用了数据并行化、模型并行化和管道并行化等并行化技术并设计了高效的训练系统，可以在16,000个GPU同时进行训练，并实现超过400 TFLOPS的计算利用率。此外，Meta还改进了硬件的可靠性和数据完整性检测机制，并引入了可扩展的存储系统，减少了检查点和回滚的开销。这些改进使得Llama 3的训练效率相较于之前的版本提高了约三倍，有效训练时间超过95%。\n指令微调 为了充分释放预训练模型在聊天场景中的潜力，Meta在指令调优的方法上进行了创新，结合了监督微调（SFT）、拒绝抽样、近端策略优化（PPO）和直接策略优化（DPO）。在SFT中使用的提示质量以及在PPO和DPO中使用的偏好排序对齐模型的性能有着重大影响。通过精心策划这些数据和进行多轮质量保证为模型的质量带来了巨大提升。\n通过PPO和DPO学习偏好排序，还极大地提高了Llama 3在推理和编码任务上的性能。Meta发现，如果向模型提出一个它难以回答的推理问题，有时模型会产生正确的推理过程：模型知道如何得出正确的答案，但不知道如何选择它。通过偏好排序的训练，模型能够学会如何进行选择。\n最后 目前， Llama3两种参数量的模型已经上线Hugging Face。Meta表示，Llama 3 模型很快将在 AWS、Databricks、Google Cloud、Hugging Face、Kaggle、IBM WatsonX、Microsoft Azure、NVIDIA NIM 和 Snowflake 上推出，并得到 AMD、AWS、Dell、Intel、NVIDIA 提供的硬件平台的支持。\n","wordCount":"111","inLanguage":"zh","datePublished":"2024-04-19T17:30:57+08:00","dateModified":"2024-04-19T17:30:57+08:00","author":{"@type":"Person","name":"Lei Xia"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.ddhigh.com/2024/04/19/meta%E5%AE%98%E5%AE%A3llama3%E8%BF%84%E4%BB%8A%E4%B8%BA%E6%AD%A2%E6%9C%80%E5%BC%BA%E5%A4%A7%E7%9A%84%E5%BC%80%E6%BA%90%E5%A4%A7%E6%A8%A1%E5%9E%8B/"},"publisher":{"@type":"Organization","name":"每天进步一点点","logo":{"@type":"ImageObject","url":"https://www.ddhigh.com/favicon.ico"}}}</script><link rel=icon href=/img/favicon.ico sizes=16x16><link rel=apple-touch-icon href=/img/favicon.ico><link rel=manifest href=/img/favicon.ico><link href=https://fonts.cdnfonts.com/css/titillium-web rel=stylesheet><link rel=stylesheet href=/css/main.min.0eb4160ba4a2d63122fe8ae83f1560951a87ab510d5dab0615973b5206555759.css integrity="sha256-DrQWC6Si1jEi/oroPxVglRqHq1ENXasGFZc7UgZVV1k=" crossorigin=anonymous media=screen><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/css/share.min.css integrity="sha512-ygEyjMC6rqnzJqWGjRTJUPYMEs9JUOm3i7OWUS9CgQ4XkBUvMsgCS1I8JqavidQ2ClHcREB7IbA2mN08+r9Elg==" crossorigin=anonymous referrerpolicy=no-referrer><link rel=stylesheet href=/scss/highlight/github-dark.min.min.66034289ee9a113219a2c4aae0a8bd2095ab255c832a42efcf5863f10814e7a1.css><script src=/js/highlight.min.min.c607d6febd16934a82eb61d3a896ed9d869f54373cc63ce95864ed5488fe3128.js></script><script>hljs.highlightAll()</script><script>$darkModeInit.Content|safeJS</script><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2871082647721658" crossorigin=anonymous></script></head><body><main class=wrapper><nav class=navigation><section class=container><a class=navigation-brand href=/>每天进步一点点
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><span></span><span></span><span></span></label><ul class=navigation-list id=navigation-list><li class="navigation-item navigation-menu"><a class=navigation-link href=/>首页</a></li><li class="navigation-item navigation-menu"><a class=navigation-link href=/archives>归档</a></li><li class="navigation-item navigation-menu"><a class=navigation-link href=/books>出版物</a></li><li class="navigation-item navigation-menu"><a class=navigation-link href=/guestbook>留言板</a></li><li class="navigation-item menu-separator"><span>|</span></li><li class="navigation-item navigation-social"><a class=navigation-link href=https://github.com/xialeistudio><svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-github"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a></li><li class="navigation-item navigation-dark"><button id=mode type=button aria-label="toggle user light or dark theme">
<span class=toggle-dark><svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-moon"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg></span>
<span class=toggle-light><svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-sun"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></span></button></li><li class="navigation-item navigation-language"><a href=https://www.ddhigh.com/en/>EN</a></li></ul></section></nav><div id=content><article class=blog-single><header class=blog-title><h1>Meta官宣Llama3：迄今为止最强大的开源大模型</h1></header><p><small>2024年4月19日&nbsp;· 111 字&nbsp;· 1 分钟</small><p><div class=blog-toc><nav id=TableOfContents><ul><li><a href=#领先的性能>领先的性能</a></li><li><a href=#三大关键设计理念>三大关键设计理念</a></li><li><a href=#最后>最后</a></li></ul></nav></div><section class=blog-content><p>4月18日，Meta在官方博客官宣了Llama3，标志着人工智能领域迈向了一个重要的飞跃。此版本具有经过预训练和指令微调的语言模型，具有 8B(80亿) 和 70B(700亿) 参数，可以支持广泛的用例。 Llama3在各种行业基准上展示了最先进的性能，并提供了新功能，包括改进的推理能力。</p><h2 id=领先的性能>领先的性能</h2><p>新的 8B 和 70B 参数 Llama 3 模型是 Llama 2 模型的重大飞跃，为这些规模的 LLM 模型确立了新的先进水平。得益于预训练和后训练的改进，Llama3的预训练和指令微调模型是目前 8B 和 70B 参数规模下最好的模型。后期训练程序的改进大大降低了错误拒绝率，提高了对齐度，并增加了模型响应的多样性。其次，推理、代码生成和指令跟踪等能力也有了很大提高，这使得 Llama 3 的可操控性更强。</p><p><img alt=438037375_405784438908376_6082258861354187544_n src=https://raw.githubusercontent.com/xialeistudio/picture-bucket/main/blog/438037375_405784438908376_6082258861354187544_n.png></p><p>在开发 Llama 3 的过程中，Meta为了优化其在真实世界场景中的性能开发了一个新的高质量人类评估集。该评估集包含 1,800 个提示，涵盖 12 种关键用例：征求建议、头脑风暴、分类、封闭式问题解答、编码、创意写作、提取、角色/人物角色、开放式问题解答、推理、改写和总结。</p><p>有意思的是，为了防止模型在这个评估集上意外地过度拟合，即使是Meta自己的建模团队也无法访问这个评估集。下图显示了针对 Claude Sonnet、Mistral Medium 和 GPT-3.5 对这些类别和提示进行的人工评估的汇总结果。</p><p><img alt=438998263_1368970367138244_7396600838045603809_n src=https://raw.githubusercontent.com/xialeistudio/picture-bucket/main/blog/438998263_1368970367138244_7396600838045603809_n.png></p><p>在与Cloud Sonnet、Mistral Medium、GPT-3.5的对比中，Llama3表现更好。</p><h2 id=三大关键设计理念>三大关键设计理念</h2><p>Meta在博客中表示，Llama 3 选择了一个相对标准的Transformer架构。与 Llama 2 相比，做了几项关键改进。Llama 3 使用了一个 128K token的tokenizer，它能更有效地编码语言，从而大幅提高模型性能。为了提高 Llama 3 模型的推理效率，Meta在 8B 和 70B 大小的模型中都采用了分组查询关注 (Grouped Query Attention, GQA)。并且在 8,192 个token的序列上对模型进行了训练，并使用掩码来确保自我关注不会跨越文档边界。</p><ol><li><strong>训练数据</strong></li></ol><p>为了训练最佳的语言模型，筛选大规模、高质量的训练数据至关重要。根据设计原则，Meta在预训练数据上进行了大量的投入。Llama 3在超过1500亿token的数据上进行了预训练，这些数据均来自公开可得的来源。<strong>Llama3的训练数据集比Llama 2使用的数据集大七倍，并且包含了四倍的代码</strong>。为了应对即将到来的多语言应用场景，Llama 3的预训练数据集中有超过<strong>5%的高质量非英语数据，涵盖了30多种语言</strong>。</p><p>为确保Llama 3的训练数据具有最高的质量，Meta开发了一系列的数据过滤流程，包括使用启发式过滤器、NSFW过滤器、语义去重方法和文本分类器来预测数据质量。Meta在博客中表示，之前的Llama版本在识别高质量数据方面表现出人意料的良好，因此他们使用Llama 2生成了训练数据，用于为驱动Llama 3的文本质量分类器提供支持。</p><p>此外，Meta还进行了大量实验使他们能够选择一种数据混合方式，以确保Llama 3在各种用例中都能表现出色，包括问答、科学技术工程数学（STEM）、编码、历史知识等。</p><ol start=2><li><strong>扩大训练规模</strong></li></ol><p>为了充分利用Llama 3模型的预训练数据，Meta致力于扩展预训练过程，并采取了多项关键措施。首先，他们开发了详细的scaling laws，通过预测最大模型在关键任务上的性能，Meta能够在实际训练之前做出明智的决策。</p><p>其次，Meta在Llama 3的开发过程中观察到了有关模型训练规模的新行为。Meta发现，即使在训练了两个数量级更多的数据后，模型的性能仍然持续改善，使他们能够构建更强大、更高效的语言模型。</p><p>为了训练最大规模的Llama 3模型，Meta采用了<strong>数据并行化、模型并行化和管道并行化</strong>等并行化技术并设计了高效的训练系统，<strong>可以在16,000个GPU同时进行训练，并实现超过400 TFLOPS的计算利用率</strong>。此外，Meta还改进了硬件的可靠性和数据完整性检测机制，并引入了可扩展的存储系统，减少了检查点和回滚的开销。这些改进使得Llama 3的训练效率相较于之前的版本提高了约三倍，有效训练时间超过95%。</p><ol start=3><li><strong>指令微调</strong></li></ol><p>为了充分释放预训练模型在聊天场景中的潜力，Meta在指令调优的方法上进行了创新，结合了<strong>监督微调（SFT）、拒绝抽样、近端策略优化（PPO）和直接策略优化（DPO）</strong>。在SFT中使用的提示质量以及在PPO和DPO中使用的偏好排序对齐模型的性能有着重大影响。通过精心策划这些数据和进行多轮质量保证为模型的质量带来了巨大提升。</p><p>通过PPO和DPO学习偏好排序，还极大地提高了Llama 3在推理和编码任务上的性能。Meta发现，如果向模型提出一个它难以回答的推理问题，有时模型会产生正确的推理过程：模型知道如何得出正确的答案，但不知道如何选择它。通过偏好排序的训练，模型能够学会如何进行选择。</p><h2 id=最后>最后</h2><p>目前， Llama3两种参数量的模型已经上线Hugging Face。Meta表示，Llama 3 模型很快将在 AWS、Databricks、Google Cloud、Hugging Face、Kaggle、IBM WatsonX、Microsoft Azure、NVIDIA NIM 和 Snowflake 上推出，并得到 AMD、AWS、Dell、Intel、NVIDIA 提供的硬件平台的支持。</p><div class=blog-footer><div class=social-share></div><div class=copyright><ul><li style=margin-bottom:.5em>本文作者: <a href=https://ddhigh.com/ target=_blank style=color:#000;text-decoration:none>xialeistudio</a></li><li style=margin-bottom:.5em>本文链接: <a href=https://www.ddhigh.com/2024/04/19/meta%E5%AE%98%E5%AE%A3llama3%E8%BF%84%E4%BB%8A%E4%B8%BA%E6%AD%A2%E6%9C%80%E5%BC%BA%E5%A4%A7%E7%9A%84%E5%BC%80%E6%BA%90%E5%A4%A7%E6%A8%A1%E5%9E%8B/ target=_blank style=color:#000;text-decoration:none>Meta官宣Llama3：迄今为止最强大的开源大模型</a></li><li>版权声明: <a href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank style=color:#000;text-decoration:none>「署名-非商业性使用-相同方式共享 4.0 国际」</a></li></ul></div><div style=margin-top:2rem><img src=/img/mp.png alt=qrcode></div></div></section><div class=paginator><a class=prev href=https://www.ddhigh.com/2024/04/20/%E8%B6%85%E8%B6%8Agpt-3.5llama3%E4%B8%AA%E4%BA%BA%E7%94%B5%E8%84%91%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E6%95%99%E7%A8%8B/><svg class="icon" width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M3.77086 21.1546C11.0491 22.698 21.4339 21.7773 21.4339 16.3608V4.63375c0-.69413-.075800000000001-1.3284-.2422-1.86588M3.77086 21.1546C1.9934 20.7777.973585 18.7264 1.08749 16.688c.17931-3.209.06972-7.25665-.08236-10.47293C.87809 3.52811 3.12891 1.16316 5.51029 1.25008c4.25565.15534 9.86671-.04779 13.28091-.24466 1.2952-.074686 2.0494.62843 2.4005 1.76245M3.77086 21.1546C4.56586 21.4723 5.49168 21.7879 6.5 22.0658M21.1917 2.76787c1.918 1.4143 1.9383 9.65123 1.7087 13.59293-2.0526 7.6586-10.5943 7.3054-16.4004 5.705M21.1917 2.76787C21.7612 4.51192 22.7203 9.67216 22 16.3608 21.2797 23.0494 11.3665 22.9511 6.5 22.0658M9.94496 9C9.28897 9.61644 7.63215 10.997 6.04814 11.7966 5.98257 11.8297 5.98456 11.9753 6.05061 12.0063c1.00435.4716 2.8788 1.9201 3.89435 2.9937M6.44444 11.9667C8.86549 12.0608 14 12 16 11" stroke="currentcolor" stroke-linecap="round"/></svg>
<span>超越GPT-3.5!Llama3个人电脑本地部署教程</span></a>
<a class=next href=https://www.ddhigh.com/2024/04/18/letter-combinations-of-a-phone-number/><span>电话号码的字母组合</span><svg class="icon" width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M3.77086 21.1546C11.0491 22.698 21.4339 21.7773 21.4339 16.3608V4.63375c0-.69413-.075800000000001-1.3284-.2422-1.86588M3.77086 21.1546C1.9934 20.7777.973585 18.7264 1.08749 16.688c.17931-3.209.06972-7.25665-.08236-10.47293C.87809 3.52811 3.12891 1.16316 5.51029 1.25008c4.25565.15534 9.86671-.04779 13.28091-.24466 1.2952-.074686 2.0494.62843 2.4005 1.76245M3.77086 21.1546C4.56586 21.4723 5.49168 21.7879 6.5 22.0658M21.1917 2.76787c1.918 1.4143 1.9383 9.65123 1.7087 13.59293-2.0526 7.6586-10.5943 7.3054-16.4004 5.705M21.1917 2.76787C21.7612 4.51192 22.7203 9.67216 22 16.3608 21.2797 23.0494 11.3665 22.9511 6.5 22.0658M12.055 9C12.711 9.61644 14.3679 10.997 15.9519 11.7966 16.0174 11.8297 16.0154 11.9753 15.9494 12.0063 14.945 12.4779 13.0706 13.9264 12.055 15m3.5006-3.0333C13.1345 12.0608 8 12 6 11" stroke="currentcolor" stroke-linecap="round"/></svg></a></div><div class=comments><script>const getTheme=window.localStorage&&window.localStorage.getItem("theme");let theme=getTheme==="dark"?"dark":"light",s=document.createElement("script");s.src="https://giscus.app/client.js",s.setAttribute("data-repo","xialeistudio/discussion"),s.setAttribute("data-repo-id","R_kgDOKurTRA"),s.setAttribute("data-category","General"),s.setAttribute("data-category-id","DIC_kwDOKurTRM4CbCJt"),s.setAttribute("data-mapping","pathname"),s.setAttribute("data-strict","0"),s.setAttribute("data-reactions-enabled","1"),s.setAttribute("data-emit-metadata","0"),s.setAttribute("data-input-position","bottom"),s.setAttribute("data-theme",theme),s.setAttribute("data-lang","en"),s.setAttribute("data-loading","lazy"),s.setAttribute("crossorigin","anonymous"),s.setAttribute("async",""),document.querySelector("div.comments").innerHTML="",document.querySelector("div.comments").appendChild(s)</script></div></article></div><footer class=footer><p>&copy; 2014 - 2024 <a href=https://www.ddhigh.com/>每天进步一点点</a>
Powered by
<a href=https://gohugo.io/ rel=noopener target=_blank>Hugo️️</a>
<a href=https://github.com/guangzhengli/hugo-theme-ladder rel=noopener target=_blank>Ladder</a>
️</p></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M10.5376 22.7916C11.0152 22.7207 22.5795 21.1781 22.0978 10.4211 22.0536 9.43274 21.9303 8.53367 21.7387 7.71865M10.5376 22.7916C16.876 22.3728 20.0969 19.8899 21.5383 16.9142M10.5376 22.7916C9.7707 22.9055 8.97982 22.8964 8.19743 22.7725M21.7387 7.71865C21.4988 6.69828 21.1518 5.80967 20.7188 5.04257m1.0199 2.67608C22.6022 10.1105 23.0542 13.7848 21.5383 16.9142M20.7188 5.04257c-3.5504-6.28886-12.88753-4.410077-16.44303.0C2.88063 6.77451-.0433281 11.1668 1.38159 16.6571c.89322 3.4417 3.7911 5.6365 6.81584 6.1154M20.7188 5.04257c1.3509 1.89783 3.3111 6.34223 1.6353 10.37273M21.5383 16.9142C21.8737 16.4251 22.1428 15.9235 22.3541 15.4153M8.19743 22.7725C12.1971 23.4683 20.6281 22.971 22.3541 15.4153M14 10.945C13.3836 10.289 12.003 8.63215 11.2034 7.04814 11.1703 6.98257 11.0247 6.98456 10.9937 7.05061 10.5221 8.05496 9.07362 9.92941 8 10.945m3.0333-3.50056C10.9392 9.86549 11 15 12 17" stroke="currentcolor" stroke-linecap="round"/></svg>
</a><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const s=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="Copy";function n(){t.innerHTML="Copied",setTimeout(()=>{t.innerHTML="Copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),n();return}const s=document.createRange();s.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(s);try{document.execCommand("copy"),n()}catch{}o.removeRange(s)}),e.parentNode.appendChild(t)})</script><script src=https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/js/social-share.min.js integrity="sha512-9DNXrSjk17bU9MUbRp3IjwcWe46V8FaGA062PFbryPUAEQVRbz4jiZP6FW0AdbqEGtMYBDWnul3eiGBMJOQajA==" crossorigin=anonymous referrerpolicy=no-referrer></script></main></body><script src=https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.6/medium-zoom.min.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ==" crossorigin=anonymous referrerpolicy=no-referrer></script><script>const images=Array.from(document.querySelectorAll(".blog-content img"));images.forEach(e=>{mediumZoom(e,{margin:10,scrollOffset:40,container:null,template:null,background:"rgba(0, 0, 0, 0.5)"})})</script><script src=/main.min.6bb26b69159420159c74dc9e097b06a578ed2b68c701466a91a44a9632d851bd0af167a1b30012387b4c512b48ad9ad4d3394e04d77ae38d57e1920fe4ed34fe.js integrity="sha512-a7JraRWUIBWcdNyeCXsGpXjtK2jHAUZqkaRKljLYUb0K8WehswASOHtMUStIrZrU0zlOBNd6441X4ZIP5O00/g==" crossorigin=anonymous defer></script></html>