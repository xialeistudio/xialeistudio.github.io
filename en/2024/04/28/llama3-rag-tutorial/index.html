<!doctype html><html lang=en><head><meta name=viewport content="width=device-width,initial-scale=1"><title>Use RAG to build your private knowledge base based on llama3 and langchain!</title>
<meta charset=utf-8><meta name=google-adsense-account content="ca-pub-2871082647721658"><meta content="Web ,Java ,Go ,Node.js ,PHP ,Koa ,MySQL ,Redis ,front-end ,back-end ,database" name=keywords><meta name=description content="LLM has timeliness and phantom problems, in How to solve the problem of large model timeliness and accuracy with? the core principle of RAG technology article I introduced the core principle of RAG, this article will share how to build a local private knowledge base based on llama3 and langchain."><meta name=author content="Lei Xia"><link rel=canonical href=https://www.ddhigh.com/en/2024/04/28/llama3-rag-tutorial/><link rel=alternate type=application/rss+xml href=https://www.ddhigh.com//index.xml title=每天进步一点点><script async defer data-website-id=52f8f0f9-d93d-466b-8ef5-508aae8c4ed4 src=https://analysis.ddhigh.com/script.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-EC3XLVSGKV"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-EC3XLVSGKV")</script><meta property="og:url" content="https://www.ddhigh.com/en/2024/04/28/llama3-rag-tutorial/"><meta property="og:site_name" content="每天进步一点点"><meta property="og:title" content="Use RAG to build your private knowledge base based on llama3 and langchain!"><meta property="og:description" content="LLM has timeliness and phantom problems, in How to solve the problem of large model timeliness and accuracy with? the core principle of RAG technology article I introduced the core principle of RAG, this article will share how to build a local private knowledge base based on llama3 and langchain."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-04-28T08:30:57+08:00"><meta property="article:modified_time" content="2024-04-28T08:30:57+08:00"><meta property="article:tag" content="Llama3"><meta property="article:tag" content="Rag"><meta name=twitter:card content="summary"><meta name=twitter:title content="Use RAG to build your private knowledge base based on llama3 and langchain!"><meta name=twitter:description content="LLM has timeliness and phantom problems, in How to solve the problem of large model timeliness and accuracy with? the core principle of RAG technology article I introduced the core principle of RAG, this article will share how to build a local private knowledge base based on llama3 and langchain."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://www.ddhigh.com/en/posts/"},{"@type":"ListItem","position":2,"name":"Use RAG to build your private knowledge base based on llama3 and langchain!","item":"https://www.ddhigh.com/en/2024/04/28/llama3-rag-tutorial/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Use RAG to build your private knowledge base based on llama3 and langchain!","name":"Use RAG to build your private knowledge base based on llama3 and langchain!","description":"LLM has timeliness and phantom problems, in How to solve the problem of large model timeliness and accuracy with? the core principle of RAG technology article I introduced the core principle of RAG, this article will share how to build a local private knowledge base based on llama3 and langchain.\n","keywords":["llama3","rag"],"articleBody":"LLM has timeliness and phantom problems, in How to solve the problem of large model timeliness and accuracy with? the core principle of RAG technology article I introduced the core principle of RAG, this article will share how to build a local private knowledge base based on llama3 and langchain.\nPrerequisites. Installation of ollama and llama3 models, see Beyond GPT-3.5! Llama3 PC Local Deployment Tutorial Install python 3.9. Install langchain for LLM coordination. Install weaviate-client for vector databases pip3 install langchain weaviate-client RAG in-action The RAG needs to retrieve the context from the vector database and then input it into the LLM for generation, so it is necessary to vectorize the text data and store it into the vector database in advance. The main steps are as follows:\nPrepare the text data chunk the text Embed and store the chunks into the vector database Create a new python3 project and index.py file, import the required modules:\nfrom langchain_community.document_loaders import TextLoader from langchain.text_splitter import CharacterTextSplitter from langchain_community.embeddings import OllamaEmbeddings import weaviate from weaviate.embedded import EmbeddedOptions from langchain.prompts import ChatPromptTemplate from langchain_community.chat_models import ChatOllama from langchain.schema.runnable import RunnablePassthrough from langchain.schema.output_parser import StrOutputParser from langchain_community.vectorstores import Weaviate import requests Download \u0026 Load Data President Biden’s 2022 State of the Union address is used here as an example. Document link https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/docs/modules/state_of_the_union.txt. langchain provides several document loaders. Here we will just use TextLoaders.\nurl = \"https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/docs/modules/state_of_the_union.txt\" res = requests.get(url) with open(\"state_of_the_union.txt\", \"w\") as f: f.write(res.text) loader = TextLoader('./state_of_the_union.txt') documents = loader.load() Chunking the corpus Since the original document is too large for LLM’s context window, it needs to be chunked in order to be recognized by LLM. LangChain provides a number of built-in text chunking tools, here we use CharacterTextSplitter as an example:\ntext_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50) chunks = text_splitter.split_documents(documents) Embedding and storing into a vector database In order to search the corpus in chunks, vectors need to be generated for each chunk and embedded in the documents, and finally the documents are stored along with the vectors. Here Ollama\u0026llama3 is used to generate the vectors and store them to Weaviate vector database.\nclient = weaviate.Client( embedded_options=EmbeddedOptions() ) print(\"store vector\") vectorstore = Weaviate.from_documents( client=client, documents=chunks, embedding=OllamaEmbeddings(model=\"llama3\"), by_text=False ) Retrieve \u0026 Enhance Once the vector database is loaded with data, it can be used as a retriever to fetch the data by semantic similarity between the user query and the embedded vector, and then just use a fixed chat template.\nretriever = vectorstore.as_retriever() template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise. Question: {question} Context: {context} Answer: \"\"\" prompt = ChatPromptTemplate.from_template(template) Generate Finally, the combination of the Retriever, the Chat Template, and the LLM into a RAG chain is all that is needed.\nllm = ChatOllama(model=\"llama3\", temperature=10) rag_chain = ( {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser() ) query = \"What did the president mainly say?\" print(rag_chain.invoke(query)) In the example above I asked LLM what the president mainly said, and LLM answered as follows:\nThe president mainly talked about continuing efforts to combat COVID-19, including vaccination rates and measures to prepare for new variants. They also discussed investments in workers, communities, and law enforcement, with a focus on fairness and justice. The tone was hopeful and emphasized the importance of taking action to improve Americans' lives. As you can see it’s still like that, LLM used the input anticipation to reply with something about the new crown epidemic as well as work, community, etc.\nlangchain supports a variety of LLMs, and readers who need it can try using the LLMs provided by OpenAI.\nReaders can construct their own private knowledge retrieval bases by replacing the lower input anticipation as needed.\nAll the code in this paper is as follows:\nfrom langchain_community.document_loaders import TextLoader from langchain.text_splitter import CharacterTextSplitter from langchain_community.embeddings import OllamaEmbeddings import weaviate from weaviate.embedded import EmbeddedOptions from langchain.prompts import ChatPromptTemplate from langchain_community.chat_models import ChatOllama from langchain.schema.runnable import RunnablePassthrough from langchain.schema.output_parser import StrOutputParser from langchain_community.vectorstores import Weaviate import requests url = \"https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/docs/modules/state_of_the_union.txt\" res = requests.get(url) with open(\"state_of_the_union.txt\", \"w\") as f: f.write(res.text) loader = TextLoader('./state_of_the_union.txt') documents = loader.load() text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50) chunks = text_splitter.split_documents(documents) client = weaviate.Client( embedded_options=EmbeddedOptions() ) vectorstore = Weaviate.from_documents( client=client, documents=chunks, embedding=OllamaEmbeddings(model=\"llama3\"), by_text=False ) retriever = vectorstore.as_retriever() template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise. Question: {question} Context: {context} Answer: \"\"\" prompt = ChatPromptTemplate.from_template(template) llm = ChatOllama(model=\"llama3\", temperature=10) rag_chain = ( {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser() ) query = \"What did the president mainly say?\" print(rag_chain.invoke(query)) ","wordCount":"806","inLanguage":"en","datePublished":"2024-04-28T08:30:57+08:00","dateModified":"2024-04-28T08:30:57+08:00","author":{"@type":"Person","name":"Lei Xia"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.ddhigh.com/en/2024/04/28/llama3-rag-tutorial/"},"publisher":{"@type":"Organization","name":"每天进步一点点","logo":{"@type":"ImageObject","url":"https://www.ddhigh.com/favicon.ico"}}}</script><link rel=icon href=/img/favicon.ico sizes=16x16><link rel=apple-touch-icon href=/img/favicon.ico><link rel=manifest href=/img/favicon.ico><link href=/titilliumweb/titilliumweb.css rel=stylesheet><link rel=stylesheet href=/css/main.min.0eb4160ba4a2d63122fe8ae83f1560951a87ab510d5dab0615973b5206555759.css integrity="sha256-DrQWC6Si1jEi/oroPxVglRqHq1ENXasGFZc7UgZVV1k=" crossorigin=anonymous media=screen><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/css/share.min.css integrity="sha512-ygEyjMC6rqnzJqWGjRTJUPYMEs9JUOm3i7OWUS9CgQ4XkBUvMsgCS1I8JqavidQ2ClHcREB7IbA2mN08+r9Elg==" crossorigin=anonymous referrerpolicy=no-referrer><link rel=stylesheet href=/scss/highlight/github-dark.min.min.66034289ee9a113219a2c4aae0a8bd2095ab255c832a42efcf5863f10814e7a1.css><script src=/js/highlight.min.min.c607d6febd16934a82eb61d3a896ed9d869f54373cc63ce95864ed5488fe3128.js></script><script>hljs.highlightAll()</script><script>$darkModeInit.Content|safeJS</script><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2871082647721658" crossorigin=anonymous></script></head><body><main class=wrapper><nav class=navigation><section class=container><a class=navigation-brand href=/en>DayDayUP
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><span></span><span></span><span></span></label><ul class=navigation-list id=navigation-list><li class="navigation-item navigation-menu"><a class=navigation-link href=/en/>Home</a></li><li class="navigation-item navigation-menu"><a class=navigation-link href=/en/archives>Archives</a></li><li class="navigation-item navigation-menu"><a class=navigation-link href=/en/books>Books</a></li><li class="navigation-item navigation-menu"><a class=navigation-link href=/en/guestbook>Guestbook</a></li><li class="navigation-item menu-separator"><span>|</span></li><li class="navigation-item navigation-social"><a class=navigation-link href=https://github.com/xialeistudio><svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-github"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a></li><li class="navigation-item navigation-dark"><button id=mode type=button aria-label="toggle user light or dark theme">
<span class=toggle-dark><svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-moon"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg></span>
<span class=toggle-light><svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-sun"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></span></button></li><li class="navigation-item navigation-language"><a href=https://www.ddhigh.com/2024/04/28/llama3-rag-tutorial/>中</a></li></ul></section></nav><div id=content><article class=blog-single><header class=blog-title><h1>Use RAG to build your private knowledge base based on llama3 and langchain!</h1></header><p><small>April 28, 2024&nbsp;· 806 words&nbsp;· 4 min</small><p><div class=blog-toc><nav id=TableOfContents><ul><li><a href=#prerequisites>Prerequisites.</a></li><li><a href=#rag-in-action>RAG in-action</a><ul><li><a href=#download--load-data>Download & Load Data</a></li><li><a href=#chunking-the-corpus>Chunking the corpus</a></li><li><a href=#embedding-and-storing-into-a-vector-database>Embedding and storing into a vector database</a></li><li><a href=#retrieve--enhance>Retrieve & Enhance</a></li><li><a href=#generate>Generate</a></li></ul></li></ul></nav></div><section class=blog-content><p>LLM has timeliness and phantom problems, in <a href=https://www.ddhigh.com/en/2024/04/24/retrieval-augmented-generation/>How to solve the problem of large model timeliness and accuracy with? the core principle of RAG technology</a> article I introduced the core principle of RAG, this article will share how to build a local private knowledge base based on llama3 and langchain.</p><h2 id=prerequisites>Prerequisites.</h2><ul><li>Installation of ollama and llama3 models, see <a href=https://www.ddhigh.com/en/2024/04/24/beyond-gpt-3.5-llama3-personal-computer-local-deployment-tutorial./>Beyond GPT-3.5! Llama3 PC Local Deployment Tutorial</a></li><li>Install python 3.9.</li><li>Install langchain for LLM coordination.</li><li>Install weaviate-client for vector databases</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip3 install langchain weaviate-client
</span></span></code></pre></div><h2 id=rag-in-action>RAG in-action</h2><p>The RAG needs to retrieve the context from the vector database and then input it into the LLM for generation, so it is necessary to vectorize the text data and store it into the vector database in advance. The main steps are as follows:</p><ol><li>Prepare the text data</li><li>chunk the text</li><li>Embed and store the chunks into the vector database</li></ol><p>Create a new python3 project and <code>index.py</code> file, import the required modules:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> langchain_community.document_loaders <span style=color:#f92672>import</span> TextLoader
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain.text_splitter <span style=color:#f92672>import</span> CharacterTextSplitter
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain_community.embeddings <span style=color:#f92672>import</span> OllamaEmbeddings
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> weaviate
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> weaviate.embedded <span style=color:#f92672>import</span> EmbeddedOptions
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain.prompts <span style=color:#f92672>import</span> ChatPromptTemplate
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain_community.chat_models <span style=color:#f92672>import</span> ChatOllama
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain.schema.runnable <span style=color:#f92672>import</span> RunnablePassthrough
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain.schema.output_parser <span style=color:#f92672>import</span> StrOutputParser
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain_community.vectorstores <span style=color:#f92672>import</span> Weaviate
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> requests
</span></span></code></pre></div><h3 id=download--load-data>Download & Load Data</h3><p>President Biden&rsquo;s 2022 State of the Union address is used here as an example. Document link <a href=https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/docs/modules/state_of_the_union.txt target=_blank rel=noopener>https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/docs/modules/state_of_the_union.txt</a>. langchain provides several document loaders. Here we will just use <code>TextLoaders</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>url <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/docs/modules/state_of_the_union.txt&#34;</span>
</span></span><span style=display:flex><span>res <span style=color:#f92672>=</span> requests<span style=color:#f92672>.</span>get(url)
</span></span><span style=display:flex><span><span style=color:#66d9ef>with</span> open(<span style=color:#e6db74>&#34;state_of_the_union.txt&#34;</span>, <span style=color:#e6db74>&#34;w&#34;</span>) <span style=color:#66d9ef>as</span> f:
</span></span><span style=display:flex><span>    f<span style=color:#f92672>.</span>write(res<span style=color:#f92672>.</span>text)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>loader <span style=color:#f92672>=</span> TextLoader(<span style=color:#e6db74>&#39;./state_of_the_union.txt&#39;</span>)
</span></span><span style=display:flex><span>documents <span style=color:#f92672>=</span> loader<span style=color:#f92672>.</span>load()
</span></span></code></pre></div><h3 id=chunking-the-corpus>Chunking the corpus</h3><p>Since the original document is too large for LLM&rsquo;s context window, it needs to be chunked in order to be recognized by LLM. LangChain provides a number of built-in text chunking tools, here we use <code>CharacterTextSplitter</code> as an example:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>text_splitter <span style=color:#f92672>=</span> CharacterTextSplitter(chunk_size<span style=color:#f92672>=</span><span style=color:#ae81ff>500</span>, chunk_overlap<span style=color:#f92672>=</span><span style=color:#ae81ff>50</span>)
</span></span><span style=display:flex><span>chunks <span style=color:#f92672>=</span> text_splitter<span style=color:#f92672>.</span>split_documents(documents)
</span></span></code></pre></div><h3 id=embedding-and-storing-into-a-vector-database>Embedding and storing into a vector database</h3><p>In order to search the corpus in chunks, vectors need to be generated for each chunk and embedded in the documents, and finally the documents are stored along with the vectors. Here Ollama&amp;llama3 is used to generate the vectors and store them to Weaviate vector database.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>client <span style=color:#f92672>=</span> weaviate<span style=color:#f92672>.</span>Client(
</span></span><span style=display:flex><span>    embedded_options<span style=color:#f92672>=</span>EmbeddedOptions()
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;store vector&#34;</span>)
</span></span><span style=display:flex><span>vectorstore <span style=color:#f92672>=</span> Weaviate<span style=color:#f92672>.</span>from_documents(
</span></span><span style=display:flex><span>    client<span style=color:#f92672>=</span>client,
</span></span><span style=display:flex><span>    documents<span style=color:#f92672>=</span>chunks,
</span></span><span style=display:flex><span>    embedding<span style=color:#f92672>=</span>OllamaEmbeddings(model<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;llama3&#34;</span>),
</span></span><span style=display:flex><span>    by_text<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><h3 id=retrieve--enhance>Retrieve & Enhance</h3><p>Once the vector database is loaded with data, it can be used as a retriever to fetch the data by semantic similarity between the user query and the embedded vector, and then just use a fixed chat template.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>retriever <span style=color:#f92672>=</span> vectorstore<span style=color:#f92672>.</span>as_retriever()
</span></span><span style=display:flex><span>template <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;&#34;You are an assistant for question-answering tasks. 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   Use the following pieces of retrieved context to answer the question. 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   If you don&#39;t know the answer, just say that you don&#39;t know. 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   Use three sentences maximum and keep the answer concise.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   Question: </span><span style=color:#e6db74>{question}</span><span style=color:#e6db74> 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   Context: </span><span style=color:#e6db74>{context}</span><span style=color:#e6db74> 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   Answer:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>prompt <span style=color:#f92672>=</span> ChatPromptTemplate<span style=color:#f92672>.</span>from_template(template)
</span></span></code></pre></div><h3 id=generate>Generate</h3><p>Finally, the combination of the Retriever, the Chat Template, and the LLM into a RAG chain is all that is needed.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>llm <span style=color:#f92672>=</span> ChatOllama(model<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;llama3&#34;</span>, temperature<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>)
</span></span><span style=display:flex><span>rag_chain <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>        {<span style=color:#e6db74>&#34;context&#34;</span>: retriever, <span style=color:#e6db74>&#34;question&#34;</span>: RunnablePassthrough()}
</span></span><span style=display:flex><span>        <span style=color:#f92672>|</span> prompt
</span></span><span style=display:flex><span>        <span style=color:#f92672>|</span> llm
</span></span><span style=display:flex><span>        <span style=color:#f92672>|</span> StrOutputParser()
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>query <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;What did the president mainly say?&#34;</span>
</span></span><span style=display:flex><span>print(rag_chain<span style=color:#f92672>.</span>invoke(query))
</span></span></code></pre></div><p>In the example above I asked LLM what the president mainly said, and LLM answered as follows:</p><pre tabindex=0><code>The president mainly talked about continuing efforts to combat COVID-19, including vaccination rates and measures to prepare for new variants. They also discussed investments in workers, communities, and law enforcement, with a focus on fairness and justice. The tone was hopeful and emphasized the importance of taking action to improve Americans&#39; lives.
</code></pre><p>As you can see it&rsquo;s still like that, LLM used the input anticipation to reply with something about the new crown epidemic as well as work, community, etc.</p><blockquote><p>langchain supports a variety of LLMs, and readers who need it can try using the LLMs provided by OpenAI.</p></blockquote><p>Readers can construct their own private knowledge retrieval bases by replacing the lower input anticipation as needed.</p><p>All the code in this paper is as follows:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> langchain_community.document_loaders <span style=color:#f92672>import</span> TextLoader
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain.text_splitter <span style=color:#f92672>import</span> CharacterTextSplitter
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain_community.embeddings <span style=color:#f92672>import</span> OllamaEmbeddings
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> weaviate
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> weaviate.embedded <span style=color:#f92672>import</span> EmbeddedOptions
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain.prompts <span style=color:#f92672>import</span> ChatPromptTemplate
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain_community.chat_models <span style=color:#f92672>import</span> ChatOllama
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain.schema.runnable <span style=color:#f92672>import</span> RunnablePassthrough
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain.schema.output_parser <span style=color:#f92672>import</span> StrOutputParser
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> langchain_community.vectorstores <span style=color:#f92672>import</span> Weaviate
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> requests
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>url <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/docs/modules/state_of_the_union.txt&#34;</span>
</span></span><span style=display:flex><span>res <span style=color:#f92672>=</span> requests<span style=color:#f92672>.</span>get(url)
</span></span><span style=display:flex><span><span style=color:#66d9ef>with</span> open(<span style=color:#e6db74>&#34;state_of_the_union.txt&#34;</span>, <span style=color:#e6db74>&#34;w&#34;</span>) <span style=color:#66d9ef>as</span> f:
</span></span><span style=display:flex><span>    f<span style=color:#f92672>.</span>write(res<span style=color:#f92672>.</span>text)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>loader <span style=color:#f92672>=</span> TextLoader(<span style=color:#e6db74>&#39;./state_of_the_union.txt&#39;</span>)
</span></span><span style=display:flex><span>documents <span style=color:#f92672>=</span> loader<span style=color:#f92672>.</span>load()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>text_splitter <span style=color:#f92672>=</span> CharacterTextSplitter(chunk_size<span style=color:#f92672>=</span><span style=color:#ae81ff>500</span>, chunk_overlap<span style=color:#f92672>=</span><span style=color:#ae81ff>50</span>)
</span></span><span style=display:flex><span>chunks <span style=color:#f92672>=</span> text_splitter<span style=color:#f92672>.</span>split_documents(documents)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>client <span style=color:#f92672>=</span> weaviate<span style=color:#f92672>.</span>Client(
</span></span><span style=display:flex><span>    embedded_options<span style=color:#f92672>=</span>EmbeddedOptions()
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>vectorstore <span style=color:#f92672>=</span> Weaviate<span style=color:#f92672>.</span>from_documents(
</span></span><span style=display:flex><span>    client<span style=color:#f92672>=</span>client,
</span></span><span style=display:flex><span>    documents<span style=color:#f92672>=</span>chunks,
</span></span><span style=display:flex><span>    embedding<span style=color:#f92672>=</span>OllamaEmbeddings(model<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;llama3&#34;</span>),
</span></span><span style=display:flex><span>    by_text<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>retriever <span style=color:#f92672>=</span> vectorstore<span style=color:#f92672>.</span>as_retriever()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>template <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;&#34;You are an assistant for question-answering tasks. 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   Use the following pieces of retrieved context to answer the question. 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   If you don&#39;t know the answer, just say that you don&#39;t know. 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   Use three sentences maximum and keep the answer concise.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   Question: </span><span style=color:#e6db74>{question}</span><span style=color:#e6db74> 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   Context: </span><span style=color:#e6db74>{context}</span><span style=color:#e6db74> 
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   Answer:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>prompt <span style=color:#f92672>=</span> ChatPromptTemplate<span style=color:#f92672>.</span>from_template(template)
</span></span><span style=display:flex><span>llm <span style=color:#f92672>=</span> ChatOllama(model<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;llama3&#34;</span>, temperature<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>)
</span></span><span style=display:flex><span>rag_chain <span style=color:#f92672>=</span> (
</span></span><span style=display:flex><span>        {<span style=color:#e6db74>&#34;context&#34;</span>: retriever, <span style=color:#e6db74>&#34;question&#34;</span>: RunnablePassthrough()}
</span></span><span style=display:flex><span>        <span style=color:#f92672>|</span> prompt
</span></span><span style=display:flex><span>        <span style=color:#f92672>|</span> llm
</span></span><span style=display:flex><span>        <span style=color:#f92672>|</span> StrOutputParser()
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>query <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;What did the president mainly say?&#34;</span>
</span></span><span style=display:flex><span>print(rag_chain<span style=color:#f92672>.</span>invoke(query))
</span></span></code></pre></div><div class=blog-footer><div class=social-share></div><div class=copyright><ul><li style=margin-bottom:.5em>Author: <a href=https://ddhigh.com/ target=_blank style=color:#000;text-decoration:none>xialeistudio</a></li><li style=margin-bottom:.5em>Link: <a href=https://www.ddhigh.com/en/2024/04/28/llama3-rag-tutorial/ target=_blank style=color:#000;text-decoration:none>Use RAG to build your private knowledge base based on llama3 and langchain!</a></li><li>Copyright: <a href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank style=color:#000;text-decoration:none>「CC BY-NC 4.0 DEED」</a></li></ul></div></div></section><div class=paginator><a class=prev href=https://www.ddhigh.com/en/2024/07/28/oauth2-comprehensive-overview/><svg class="icon" width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M3.77086 21.1546C11.0491 22.698 21.4339 21.7773 21.4339 16.3608V4.63375c0-.69413-.075800000000001-1.3284-.2422-1.86588M3.77086 21.1546C1.9934 20.7777.973585 18.7264 1.08749 16.688c.17931-3.209.06972-7.25665-.08236-10.47293C.87809 3.52811 3.12891 1.16316 5.51029 1.25008c4.25565.15534 9.86671-.04779 13.28091-.24466 1.2952-.074686 2.0494.62843 2.4005 1.76245M3.77086 21.1546C4.56586 21.4723 5.49168 21.7879 6.5 22.0658M21.1917 2.76787c1.918 1.4143 1.9383 9.65123 1.7087 13.59293-2.0526 7.6586-10.5943 7.3054-16.4004 5.705M21.1917 2.76787C21.7612 4.51192 22.7203 9.67216 22 16.3608 21.2797 23.0494 11.3665 22.9511 6.5 22.0658M9.94496 9C9.28897 9.61644 7.63215 10.997 6.04814 11.7966 5.98257 11.8297 5.98456 11.9753 6.05061 12.0063c1.00435.4716 2.8788 1.9201 3.89435 2.9937M6.44444 11.9667C8.86549 12.0608 14 12 16 11" stroke="currentcolor" stroke-linecap="round"/></svg>
<span>A Comprehensive Overview of The Oauth2.0 Framework</span></a>
<a class=next href=https://www.ddhigh.com/en/2024/04/24/retrieval-augmented-generation/><span>Retrieval-Augmented Generation</span><svg class="icon" width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M3.77086 21.1546C11.0491 22.698 21.4339 21.7773 21.4339 16.3608V4.63375c0-.69413-.075800000000001-1.3284-.2422-1.86588M3.77086 21.1546C1.9934 20.7777.973585 18.7264 1.08749 16.688c.17931-3.209.06972-7.25665-.08236-10.47293C.87809 3.52811 3.12891 1.16316 5.51029 1.25008c4.25565.15534 9.86671-.04779 13.28091-.24466 1.2952-.074686 2.0494.62843 2.4005 1.76245M3.77086 21.1546C4.56586 21.4723 5.49168 21.7879 6.5 22.0658M21.1917 2.76787c1.918 1.4143 1.9383 9.65123 1.7087 13.59293-2.0526 7.6586-10.5943 7.3054-16.4004 5.705M21.1917 2.76787C21.7612 4.51192 22.7203 9.67216 22 16.3608 21.2797 23.0494 11.3665 22.9511 6.5 22.0658M12.055 9C12.711 9.61644 14.3679 10.997 15.9519 11.7966 16.0174 11.8297 16.0154 11.9753 15.9494 12.0063 14.945 12.4779 13.0706 13.9264 12.055 15m3.5006-3.0333C13.1345 12.0608 8 12 6 11" stroke="currentcolor" stroke-linecap="round"/></svg></a></div><div class=comments><script>const getTheme=window.localStorage&&window.localStorage.getItem("theme");let theme=getTheme==="dark"?"dark":"light",s=document.createElement("script");s.src="https://giscus.app/client.js",s.setAttribute("data-repo","xialeistudio/discussion"),s.setAttribute("data-repo-id","R_kgDOKurTRA"),s.setAttribute("data-category","General"),s.setAttribute("data-category-id","DIC_kwDOKurTRM4CbCJt"),s.setAttribute("data-mapping","pathname"),s.setAttribute("data-strict","0"),s.setAttribute("data-reactions-enabled","1"),s.setAttribute("data-emit-metadata","0"),s.setAttribute("data-input-position","bottom"),s.setAttribute("data-theme",theme),s.setAttribute("data-lang","en"),s.setAttribute("data-loading","lazy"),s.setAttribute("crossorigin","anonymous"),s.setAttribute("async",""),document.querySelector("div.comments").innerHTML="",document.querySelector("div.comments").appendChild(s)</script></div></article></div><footer class=footer><p>&copy; 2014 - 2024 <a href=https://www.ddhigh.com/>每天进步一点点</a>
Powered by
<a href=https://gohugo.io/ rel=noopener target=_blank>Hugo️️</a>
<a href=https://github.com/guangzhengli/hugo-theme-ladder rel=noopener target=_blank>Ladder</a>
️</p></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M10.5376 22.7916C11.0152 22.7207 22.5795 21.1781 22.0978 10.4211 22.0536 9.43274 21.9303 8.53367 21.7387 7.71865M10.5376 22.7916C16.876 22.3728 20.0969 19.8899 21.5383 16.9142M10.5376 22.7916C9.7707 22.9055 8.97982 22.8964 8.19743 22.7725M21.7387 7.71865C21.4988 6.69828 21.1518 5.80967 20.7188 5.04257m1.0199 2.67608C22.6022 10.1105 23.0542 13.7848 21.5383 16.9142M20.7188 5.04257c-3.5504-6.28886-12.88753-4.410077-16.44303.0C2.88063 6.77451-.0433281 11.1668 1.38159 16.6571c.89322 3.4417 3.7911 5.6365 6.81584 6.1154M20.7188 5.04257c1.3509 1.89783 3.3111 6.34223 1.6353 10.37273M21.5383 16.9142C21.8737 16.4251 22.1428 15.9235 22.3541 15.4153M8.19743 22.7725C12.1971 23.4683 20.6281 22.971 22.3541 15.4153M14 10.945C13.3836 10.289 12.003 8.63215 11.2034 7.04814 11.1703 6.98257 11.0247 6.98456 10.9937 7.05061 10.5221 8.05496 9.07362 9.92941 8 10.945m3.0333-3.50056C10.9392 9.86549 11 15 12 17" stroke="currentcolor" stroke-linecap="round"/></svg>
</a><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const s=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="Copy";function n(){t.innerHTML="Copied",setTimeout(()=>{t.innerHTML="Copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),n();return}const s=document.createRange();s.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(s);try{document.execCommand("copy"),n()}catch{}o.removeRange(s)}),e.parentNode.appendChild(t)})</script><script src=https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/js/social-share.min.js integrity="sha512-9DNXrSjk17bU9MUbRp3IjwcWe46V8FaGA062PFbryPUAEQVRbz4jiZP6FW0AdbqEGtMYBDWnul3eiGBMJOQajA==" crossorigin=anonymous referrerpolicy=no-referrer></script></main></body><script src=https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.6/medium-zoom.min.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ==" crossorigin=anonymous referrerpolicy=no-referrer></script><script>const images=Array.from(document.querySelectorAll(".blog-content img"));images.forEach(e=>{mediumZoom(e,{margin:10,scrollOffset:40,container:null,template:null,background:"rgba(0, 0, 0, 0.5)"})})</script><script src=/main.min.6bb26b69159420159c74dc9e097b06a578ed2b68c701466a91a44a9632d851bd0af167a1b30012387b4c512b48ad9ad4d3394e04d77ae38d57e1920fe4ed34fe.js integrity="sha512-a7JraRWUIBWcdNyeCXsGpXjtK2jHAUZqkaRKljLYUb0K8WehswASOHtMUStIrZrU0zlOBNd6441X4ZIP5O00/g==" crossorigin=anonymous defer></script></html>