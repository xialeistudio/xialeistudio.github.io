<!doctype html><html lang=en><head><meta name=viewport content="width=device-width,initial-scale=1"><title>Beyond GPT-3.5! Llama3 Personal Computer Local Deployment Tutorial.</title>
<meta charset=utf-8><meta name=google-adsense-account content="ca-pub-2871082647721658"><meta content="Web ,Java ,Go ,Node.js ,PHP ,Koa ,MySQL ,Redis ,front-end ,back-end ,database" name=keywords><meta name=description content="On April 18th, Meta announced Llama3 on their official blog, marking a significant leap in the field of artificial intelligence."><meta name=author content="Lei Xia"><link rel=canonical href=https://www.ddhigh.com/en/2024/04/24/beyond-gpt-3.5-llama3-personal-computer-local-deployment-tutorial./><link rel=alternate type=application/rss+xml href=https://www.ddhigh.com//index.xml title=每天进步一点点><script async defer data-website-id=52f8f0f9-d93d-466b-8ef5-508aae8c4ed4 src=https://analysis.ddhigh.com/script.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-EC3XLVSGKV"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-EC3XLVSGKV")</script><meta property="og:url" content="https://www.ddhigh.com/en/2024/04/24/beyond-gpt-3.5-llama3-personal-computer-local-deployment-tutorial./"><meta property="og:site_name" content="每天进步一点点"><meta property="og:title" content="Beyond GPT-3.5! Llama3 Personal Computer Local Deployment Tutorial."><meta property="og:description" content="On April 18th, Meta announced Llama3 on their official blog, marking a significant leap in the field of artificial intelligence."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-04-24T19:30:57+08:00"><meta property="article:modified_time" content="2024-04-24T19:30:57+08:00"><meta property="article:tag" content="Llama3"><meta name=twitter:card content="summary"><meta name=twitter:title content="Beyond GPT-3.5! Llama3 Personal Computer Local Deployment Tutorial."><meta name=twitter:description content="On April 18th, Meta announced Llama3 on their official blog, marking a significant leap in the field of artificial intelligence."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://www.ddhigh.com/en/posts/"},{"@type":"ListItem","position":2,"name":"Beyond GPT-3.5! Llama3 Personal Computer Local Deployment Tutorial.","item":"https://www.ddhigh.com/en/2024/04/24/beyond-gpt-3.5-llama3-personal-computer-local-deployment-tutorial./"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Beyond GPT-3.5! Llama3 Personal Computer Local Deployment Tutorial.","name":"Beyond GPT-3.5! Llama3 Personal Computer Local Deployment Tutorial.","description":"On April 18th, Meta announced Llama3 on their official blog, marking a significant leap in the field of artificial intelligence.","keywords":["llama3"],"articleBody":"On April 18th, Meta announced Llama3 on their official blog, marking a significant leap in the field of artificial intelligence. Based on my personal experience, Llama3’s 8B model has surpassed GPT-3.5, and most importantly, Llama3 is open source, allowing us to deploy it on our own!\nIn this article, I will share how to deploy Llama3 on a personal computer, giving you your own GPT-3.5+!\nMany readers may be concerned that their personal computer’s hardware configuration is insufficient for local deployment. However, this concern is unnecessary. For reference, I used a MacBook M2 Pro (2023 model) with the following primary hardware specifications:\n10-core CPU 16 GB RAM The deployment steps are roughly as follows:\nInstall Ollama Download Llama3 Install Node.js Deploy the WebUI Install Ollama Ollama can be simply understood as a client, which enables interaction with large models. Readers can go to https://ollama.com/download to download the corresponding client according to the operating system type. For example, I downloaded macOS.\nAfter downloading, open it and click Next and Install directly to install ollama to the command line. After the installation is complete, the interface will prompt ollama run llama2. There is no need to execute this command because we want to install llama3.\nDownload Llama3 Open new CMD/Terminal, execute the following command:\nollama run llama3 The program will automatically download the Llama3 model file. The default is 8B, which is an 8 billion parameter version. It can be run on a personal computer.\nAfter successfully downloading the model, you will enter the interactive interface. We can ask questions directly on the terminal. For example, I asked Who are you?, and Llama3 answered it in almost seconds.\n➜ Projects ollama run llama3 \u003e\u003e\u003e who are you? I'm LLaMA, a large language model trained by a team of researcher at Meta AI. I'm here to chat with you and answer any questions you may have. I've been trained on a massive dataset of text from the internet and can generate human-like responses to a wide range of topics and questions. My training data includes but is not limited to: * Web pages * Books * Articles * Research papers * Conversations I'm constantly learning and improving my responses based on the conversations I have with users like you. So, what's on your mind? Do you have a question or topic you'd like to discuss? Install Node.js There are many WebUIs that support Ollama. I have experienced the most popular WebUI (https://github.com/open-webui/open-webui), which requires Docker or Kubernetes deployment, which is a bit troublesome, and the image is about 1G.\nI recommend using ollama-webui-lite (https://github.com/ollama-webui/ollama-webui-lite), which is very lightweight and only relies on Node.js.\nYou can go to (https://nodejs.org/en/download) to download the corresponding Node.js according to their operating system and CPU chip type and install it.\nWebUI Deployment Open CMD/Terminal, exectute the following commands to deploy the WebUI:\ngit clone https://github.com/ollama-webui/ollama-webui-lite.git cd ollama-webui-lite npm install npm run dev As we can see, WebUI is already listening on the local port 3000:\n\u003e ollama-webui-lite@0.0.1 dev \u003e vite dev --host --port 3000 VITE v4.5.2 ready in 765 ms ➜ Local: http://localhost:3000/ Open the browser and visit http://localhost:3000, you can see the screenshot as shown below. By default, the model is not selected. You need to click on the arrow shown in the screenshot to select the model.\nI gave the model an example of writing a Golang Echo Server, and it started printing results in about 5 seconds, which is very fast.\nAny questions? Leave a comment!\n","wordCount":"581","inLanguage":"en","datePublished":"2024-04-24T19:30:57+08:00","dateModified":"2024-04-24T19:30:57+08:00","author":{"@type":"Person","name":"Lei Xia"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://www.ddhigh.com/en/2024/04/24/beyond-gpt-3.5-llama3-personal-computer-local-deployment-tutorial./"},"publisher":{"@type":"Organization","name":"每天进步一点点","logo":{"@type":"ImageObject","url":"https://www.ddhigh.com/favicon.ico"}}}</script><link rel=icon href=/img/favicon.ico sizes=16x16><link rel=apple-touch-icon href=/img/favicon.ico><link rel=manifest href=/img/favicon.ico><link rel=stylesheet href=https://cdn.staticfile.org/lxgw-wenkai-webfont/1.6.0/style.css><link rel=stylesheet href=/css/main.min.d0df733b91ef046970ecd63de7aa626886fd263fd3fb2b8ff28b560e505831f7.css integrity="sha256-0N9zO5HvBGlw7NY956piaIb9Jj/T+yuP8otWDlBYMfc=" crossorigin=anonymous media=screen><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/css/share.min.css integrity="sha512-ygEyjMC6rqnzJqWGjRTJUPYMEs9JUOm3i7OWUS9CgQ4XkBUvMsgCS1I8JqavidQ2ClHcREB7IbA2mN08+r9Elg==" crossorigin=anonymous referrerpolicy=no-referrer><link rel=stylesheet href=/scss/highlight/github-dark.min.min.66034289ee9a113219a2c4aae0a8bd2095ab255c832a42efcf5863f10814e7a1.css><script src=/js/highlight.min.min.c607d6febd16934a82eb61d3a896ed9d869f54373cc63ce95864ed5488fe3128.js></script><script>hljs.highlightAll()</script><script>$darkModeInit.Content|safeJS</script></head><body><main class=wrapper><nav class=navigation><section class=container><a class=navigation-brand href=/en>DayDayUP
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><span></span><span></span><span></span></label><ul class=navigation-list id=navigation-list><li class="navigation-item navigation-menu"><a class=navigation-link href=/en/>Home</a></li><li class="navigation-item navigation-menu"><a class=navigation-link href=/en/archives>Archives</a></li><li class="navigation-item navigation-menu"><a class=navigation-link href=/en/books>Books</a></li><li class="navigation-item navigation-menu"><a class=navigation-link href=/en/guestbook>Guestbook</a></li><li class="navigation-item menu-separator"><span>|</span></li><li class="navigation-item navigation-social"><a class=navigation-link href=https://github.com/xialeistudio><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-github"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a></li><li class="navigation-item navigation-dark"><button id=mode type=button aria-label="toggle user light or dark theme">
<span class=toggle-dark><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-moon"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg></span>
<span class=toggle-light><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-sun"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></span></button></li><li class="navigation-item navigation-language"><a href=https://www.ddhigh.com/>中</a></li></ul></section></nav><div id=content><article class=blog-single><header class=blog-title><h1>Beyond GPT-3.5! Llama3 Personal Computer Local Deployment Tutorial.</h1></header><p><small>April 24, 2024&nbsp;· 581 words&nbsp;· 3 min</small><p><div class=blog-toc><nav id=TableOfContents><ul><li><a href=#install-ollama>Install Ollama</a></li><li><a href=#download-llama3>Download Llama3</a></li><li><a href=#install-nodejs>Install Node.js</a></li><li><a href=#webui-deployment>WebUI Deployment</a></li></ul></nav></div><section class=blog-content><p>On April 18th, Meta announced Llama3 on their official blog, marking a significant leap in the field of artificial intelligence. Based on my personal experience, Llama3&rsquo;s 8B model has surpassed GPT-3.5, and most importantly, Llama3 is open source, allowing us to deploy it on our own!</p><p>In this article, I will share how to deploy Llama3 on a personal computer, giving you your own GPT-3.5+!</p><p>Many readers may be concerned that their personal computer&rsquo;s hardware configuration is insufficient for local deployment. However, this concern is unnecessary. For reference, I used a MacBook M2 Pro (2023 model) with the following primary hardware specifications:</p><ul><li>10-core CPU</li><li>16 GB RAM</li></ul><p>The deployment steps are roughly as follows:</p><ul><li>Install Ollama</li><li>Download Llama3</li><li>Install Node.js</li><li>Deploy the WebUI</li></ul><h2 id=install-ollama>Install Ollama</h2><p>Ollama can be simply understood as a client, which enables interaction with large models. Readers can go to <a href=https://ollama.com/download target=_blank rel=noopener>https://ollama.com/download</a> to download the corresponding client according to the operating system type. For example, I downloaded macOS.</p><p><img alt=WX20240420-085342@2x src=https://raw.githubusercontent.com/xialeistudio/picture-bucket/main/blog/WX20240420-085342%402x.png></p><p>After downloading, open it and click <code>Next</code> and <code>Install</code> directly to install <code>ollama</code> to the command line. After the installation is complete, the interface will prompt <code>ollama run llama2</code>. There is no need to execute this command because we want to install <code>llama3</code>.</p><p><img alt=image-20240420085557726 src=https://raw.githubusercontent.com/xialeistudio/picture-bucket/main/blog/image-20240420085557726.png></p><h2 id=download-llama3>Download Llama3</h2><p>Open new CMD/Terminal, execute the following command:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ollama run llama3
</span></span></code></pre></div><p>The program will automatically download the Llama3 model file. The default is 8B, which is an 8 billion parameter version. It can be run on a personal computer.</p><p>After successfully downloading the model, you will enter the interactive interface. We can ask questions directly on the terminal. For example, I asked <code>Who are you?</code>, and Llama3 answered it in almost seconds.</p><pre tabindex=0><code>➜  Projects ollama run llama3
&gt;&gt;&gt; who are you?
I&#39;m LLaMA, a large language model trained by a team of researcher at Meta 
AI. I&#39;m here to chat with you and answer any questions you may have.

I&#39;ve been trained on a massive dataset of text from the internet and can 
generate human-like responses to a wide range of topics and questions. My 
training data includes but is not limited to:

* Web pages
* Books
* Articles
* Research papers
* Conversations

I&#39;m constantly learning and improving my responses based on the 
conversations I have with users like you.

So, what&#39;s on your mind? Do you have a question or topic you&#39;d like to 
discuss?
</code></pre><h2 id=install-nodejs>Install Node.js</h2><p>There are many WebUIs that support Ollama. I have experienced the most popular WebUI (<a href=https://github.com/open-webui/open-webui%29 target=_blank rel=noopener>https://github.com/open-webui/open-webui)</a>, which requires Docker or Kubernetes deployment, which is a bit troublesome, and the image is about 1G.</p><p>I recommend using ollama-webui-lite (<a href=https://github.com/ollama-webui/ollama-webui-lite%29 target=_blank rel=noopener>https://github.com/ollama-webui/ollama-webui-lite)</a>, which is very lightweight and only relies on Node.js.</p><p>You can go to (<a href=https://nodejs.org/en/download target=_blank rel=noopener>https://nodejs.org/en/download</a>) to download the corresponding Node.js according to their operating system and CPU chip type and install it.</p><p><img alt=image-20240420090338877 src=https://raw.githubusercontent.com/xialeistudio/picture-bucket/main/blog/image-20240420090338877.png></p><h2 id=webui-deployment>WebUI Deployment</h2><p>Open CMD/Terminal, exectute the following commands to deploy the WebUI:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git clone https://github.com/ollama-webui/ollama-webui-lite.git
</span></span><span style=display:flex><span>cd ollama-webui-lite
</span></span><span style=display:flex><span>npm install
</span></span><span style=display:flex><span>npm run dev
</span></span></code></pre></div><p>As we can see, WebUI is already listening on the local port 3000:</p><pre tabindex=0><code>&gt; ollama-webui-lite@0.0.1 dev
&gt; vite dev --host --port 3000



  VITE v4.5.2  ready in 765 ms

  ➜  Local:   http://localhost:3000/
</code></pre><p>Open the browser and visit http://localhost:3000, you can see the screenshot as shown below. By default, the model is not selected. You need to click on the arrow shown in the screenshot to select the model.</p><p><img alt=image-20240420091143684 src=https://raw.githubusercontent.com/xialeistudio/picture-bucket/main/blog/image-20240420091143684.png></p><p>I gave the model an example of writing a Golang Echo Server, and it started printing results in about 5 seconds, which is very fast.</p><p><img alt=image-20240420091325732 src=https://raw.githubusercontent.com/xialeistudio/picture-bucket/main/blog/image-20240420091325732.png></p><p>Any questions? Leave a comment!</p><div class=blog-footer><div class=social-share></div><div class=copyright><ul><li style=margin-bottom:.5em>Author: <a href=https://ddhigh.com/ target=_blank style=color:#000;text-decoration:none>xialeistudio</a></li><li style=margin-bottom:.5em>Link: <a href=https://www.ddhigh.com/en/2024/04/24/beyond-gpt-3.5-llama3-personal-computer-local-deployment-tutorial./ target=_blank style=color:#000;text-decoration:none>Beyond GPT-3.5! Llama3 Personal Computer Local Deployment Tutorial.</a></li><li>Copyright: <a href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank style=color:#000;text-decoration:none>「CC BY-NC 4.0 DEED」</a></li></ul></div></div></section><div class=paginator><a class=prev href=https://www.ddhigh.com/en/2024/04/24/retrieval-augmented-generation/><svg class="icon" width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M3.77086 21.1546C11.0491 22.698 21.4339 21.7773 21.4339 16.3608V4.63375c0-.69413-.075800000000001-1.3284-.2422-1.86588M3.77086 21.1546C1.9934 20.7777.973585 18.7264 1.08749 16.688c.17931-3.209.06972-7.25665-.08236-10.47293C.87809 3.52811 3.12891 1.16316 5.51029 1.25008c4.25565.15534 9.86671-.04779 13.28091-.24466 1.2952-.074686 2.0494.62843 2.4005 1.76245M3.77086 21.1546C4.56586 21.4723 5.49168 21.7879 6.5 22.0658M21.1917 2.76787c1.918 1.4143 1.9383 9.65123 1.7087 13.59293-2.0526 7.6586-10.5943 7.3054-16.4004 5.705M21.1917 2.76787C21.7612 4.51192 22.7203 9.67216 22 16.3608 21.2797 23.0494 11.3665 22.9511 6.5 22.0658M9.94496 9C9.28897 9.61644 7.63215 10.997 6.04814 11.7966 5.98257 11.8297 5.98456 11.9753 6.05061 12.0063c1.00435.4716 2.8788 1.9201 3.89435 2.9937M6.44444 11.9667C8.86549 12.0608 14 12 16 11" stroke="currentcolor" stroke-linecap="round"/></svg>
<span>Retrieval-Augmented Generation</span></a>
<a class=next href=https://www.ddhigh.com/en/2024/03/17/understanding-replication-in-mysql/><span>Understanding Replication In MySQL</span><svg class="icon" width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M3.77086 21.1546C11.0491 22.698 21.4339 21.7773 21.4339 16.3608V4.63375c0-.69413-.075800000000001-1.3284-.2422-1.86588M3.77086 21.1546C1.9934 20.7777.973585 18.7264 1.08749 16.688c.17931-3.209.06972-7.25665-.08236-10.47293C.87809 3.52811 3.12891 1.16316 5.51029 1.25008c4.25565.15534 9.86671-.04779 13.28091-.24466 1.2952-.074686 2.0494.62843 2.4005 1.76245M3.77086 21.1546C4.56586 21.4723 5.49168 21.7879 6.5 22.0658M21.1917 2.76787c1.918 1.4143 1.9383 9.65123 1.7087 13.59293-2.0526 7.6586-10.5943 7.3054-16.4004 5.705M21.1917 2.76787C21.7612 4.51192 22.7203 9.67216 22 16.3608 21.2797 23.0494 11.3665 22.9511 6.5 22.0658M12.055 9C12.711 9.61644 14.3679 10.997 15.9519 11.7966 16.0174 11.8297 16.0154 11.9753 15.9494 12.0063 14.945 12.4779 13.0706 13.9264 12.055 15m3.5006-3.0333C13.1345 12.0608 8 12 6 11" stroke="currentcolor" stroke-linecap="round"/></svg></a></div><div class=comments><script>const getTheme=window.localStorage&&window.localStorage.getItem("theme");let theme=getTheme==="dark"?"dark":"light",s=document.createElement("script");s.src="https://giscus.app/client.js",s.setAttribute("data-repo","xialeistudio/discussion"),s.setAttribute("data-repo-id","R_kgDOKurTRA"),s.setAttribute("data-category","General"),s.setAttribute("data-category-id","DIC_kwDOKurTRM4CbCJt"),s.setAttribute("data-mapping","pathname"),s.setAttribute("data-strict","0"),s.setAttribute("data-reactions-enabled","1"),s.setAttribute("data-emit-metadata","0"),s.setAttribute("data-input-position","bottom"),s.setAttribute("data-theme",theme),s.setAttribute("data-lang","en"),s.setAttribute("data-loading","lazy"),s.setAttribute("crossorigin","anonymous"),s.setAttribute("async",""),document.querySelector("div.comments").innerHTML="",document.querySelector("div.comments").appendChild(s)</script></div></article></div><footer class=footer><p>&copy; 2014 - 2024 <a href=https://www.ddhigh.com/>每天进步一点点</a>
Powered by
<a href=https://gohugo.io/ rel=noopener target=_blank>Hugo️️</a>
<a href=https://github.com/guangzhengli/hugo-theme-ladder rel=noopener target=_blank>Ladder</a>
️</p></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M10.5376 22.7916C11.0152 22.7207 22.5795 21.1781 22.0978 10.4211 22.0536 9.43274 21.9303 8.53367 21.7387 7.71865M10.5376 22.7916C16.876 22.3728 20.0969 19.8899 21.5383 16.9142M10.5376 22.7916C9.7707 22.9055 8.97982 22.8964 8.19743 22.7725M21.7387 7.71865C21.4988 6.69828 21.1518 5.80967 20.7188 5.04257m1.0199 2.67608C22.6022 10.1105 23.0542 13.7848 21.5383 16.9142M20.7188 5.04257c-3.5504-6.28886-12.88753-4.410077-16.44303.0C2.88063 6.77451-.0433281 11.1668 1.38159 16.6571c.89322 3.4417 3.7911 5.6365 6.81584 6.1154M20.7188 5.04257c1.3509 1.89783 3.3111 6.34223 1.6353 10.37273M21.5383 16.9142C21.8737 16.4251 22.1428 15.9235 22.3541 15.4153M8.19743 22.7725C12.1971 23.4683 20.6281 22.971 22.3541 15.4153M14 10.945C13.3836 10.289 12.003 8.63215 11.2034 7.04814 11.1703 6.98257 11.0247 6.98456 10.9937 7.05061 10.5221 8.05496 9.07362 9.92941 8 10.945m3.0333-3.50056C10.9392 9.86549 11 15 12 17" stroke="currentcolor" stroke-linecap="round"/></svg>
</a><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const s=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="Copy";function n(){t.innerHTML="Copied",setTimeout(()=>{t.innerHTML="Copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),n();return}const s=document.createRange();s.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(s);try{document.execCommand("copy"),n()}catch{}o.removeRange(s)}),e.parentNode.appendChild(t)})</script><script src=https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/js/social-share.min.js integrity="sha512-9DNXrSjk17bU9MUbRp3IjwcWe46V8FaGA062PFbryPUAEQVRbz4jiZP6FW0AdbqEGtMYBDWnul3eiGBMJOQajA==" crossorigin=anonymous referrerpolicy=no-referrer></script></main></body><script src=https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.6/medium-zoom.min.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ==" crossorigin=anonymous referrerpolicy=no-referrer></script><script>const images=Array.from(document.querySelectorAll(".blog-content img"));images.forEach(e=>{mediumZoom(e,{margin:10,scrollOffset:40,container:null,template:null,background:"rgba(0, 0, 0, 0.5)"})})</script><script src=/main.min.6bb26b69159420159c74dc9e097b06a578ed2b68c701466a91a44a9632d851bd0af167a1b30012387b4c512b48ad9ad4d3394e04d77ae38d57e1920fe4ed34fe.js integrity="sha512-a7JraRWUIBWcdNyeCXsGpXjtK2jHAUZqkaRKljLYUb0K8WehswASOHtMUStIrZrU0zlOBNd6441X4ZIP5O00/g==" crossorigin=anonymous defer></script></html>